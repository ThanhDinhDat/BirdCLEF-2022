{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General\n",
    "- Baseline with naive solution Xresnet34\n",
    "- Best Acc: 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-02-24T15:48:04.465475Z",
     "iopub.status.busy": "2022-02-24T15:48:04.464774Z",
     "iopub.status.idle": "2022-02-24T15:48:08.071660Z",
     "shell.execute_reply": "2022-02-24T15:48:08.071120Z",
     "shell.execute_reply.started": "2022-02-24T15:32:42.184264Z"
    },
    "papermill": {
     "duration": 4.842793,
     "end_time": "2022-02-24T15:48:08.071818",
     "exception": false,
     "start_time": "2022-02-24T15:48:03.229025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kitemetric/workspace/kaggle-challenges/BirdCLEF-2022/.venv/lib/python3.8/site-packages/torchaudio/backend/utils.py:46: UserWarning: \"torchaudio.USE_SOUNDFILE_LEGACY_INTERFACE\" flag is deprecated and will be removed in 0.9.0. Please remove the use of flag.\n",
      "  warnings.warn(\n",
      "/home/kitemetric/workspace/kaggle-challenges/BirdCLEF-2022/.venv/lib/python3.8/site-packages/torchaudio/backend/utils.py:46: UserWarning: \"torchaudio.USE_SOUNDFILE_LEGACY_INTERFACE\" flag is deprecated and will be removed in 0.9.0. Please remove the use of flag.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import noisereduce as nr\n",
    "\n",
    "# import modin.pandas as pd\n",
    "import pandas as pd\n",
    "from fastaudio.all import *\n",
    "from fastai.vision.all import *\n",
    "\n",
    "import torch\n",
    "import fastcore\n",
    "import fastai\n",
    "import torchaudio\n",
    "torchaudio.set_audio_backend(\"sox_io\")\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.041219,
     "end_time": "2022-02-24T15:48:12.454915",
     "exception": false,
     "start_time": "2022-02-24T15:48:11.413696",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "------------------------------------------------------\n",
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-24T15:48:14.562915Z",
     "iopub.status.busy": "2022-02-24T15:48:14.562345Z",
     "iopub.status.idle": "2022-02-24T15:48:14.565802Z",
     "shell.execute_reply": "2022-02-24T15:48:14.566445Z",
     "shell.execute_reply.started": "2022-02-24T15:32:46.027048Z"
    },
    "papermill": {
     "duration": 1.058616,
     "end_time": "2022-02-24T15:48:14.566663",
     "exception": false,
     "start_time": "2022-02-24T15:48:13.508047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CONFIGURATIONS\n",
    "IS_KAGGLE = False\n",
    "if IS_KAGGLE:\n",
    "    ROOT_DIR = Path('../../input/birdclef-2022')\n",
    "else:\n",
    "    ROOT_DIR = Path('../../datasets/birdclef-2022')\n",
    "DATA_DIR = ROOT_DIR / \"train_audio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-24T15:48:16.819402Z",
     "iopub.status.busy": "2022-02-24T15:48:16.818617Z",
     "iopub.status.idle": "2022-02-24T15:48:18.208647Z",
     "shell.execute_reply": "2022-02-24T15:48:18.209216Z",
     "shell.execute_reply.started": "2022-02-24T15:32:46.032924Z"
    },
    "papermill": {
     "duration": 2.482467,
     "end_time": "2022-02-24T15:48:18.209407",
     "exception": false,
     "start_time": "2022-02-24T15:48:15.726940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of audio files: 14852\n",
      "Number of test files: 3\n"
     ]
    }
   ],
   "source": [
    "audio_fns = get_audio_files(DATA_DIR)\n",
    "print(f'No. of audio files: {len(audio_fns)}')\n",
    "\n",
    "test_df = pd.read_csv(ROOT_DIR / \"test.csv\")\n",
    "print(f\"Number of test files: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87fc000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "\n",
    "def crop_or_pad(y, length, sr, train=True, offset=0, probs=None):\n",
    "    \"\"\"\n",
    "    Crops an array to a chosen length\n",
    "    Arguments:\n",
    "        y {1D np array} -- Array to crop\n",
    "        length {int} -- Length of the crop\n",
    "        sr {int} -- Sampling rate\n",
    "    Keyword Arguments:\n",
    "        train {bool} -- Whether we are at train time. If so, crop randomly, else return the beginning of y (default: {True})\n",
    "        probs {None or numpy array} -- Probabilities to use to chose where to crop (default: {None})\n",
    "    Returns:\n",
    "        1D np array -- Cropped array\n",
    "    \"\"\"\n",
    "    if len(y) <= length:\n",
    "        y = np.concatenate([y, np.zeros(length - len(y))])\n",
    "    else:\n",
    "        if not train:\n",
    "            start = 0 + offset\n",
    "        elif probs is None:\n",
    "            start = np.random.randint(len(y) - length)\n",
    "        else:\n",
    "            start = (\n",
    "                    np.random.choice(np.arange(len(probs)), p=probs) + np.random.random()\n",
    "            )\n",
    "            start = int(sr * (start))\n",
    "\n",
    "        y = y[start: start + length]\n",
    "\n",
    "    return torch.from_numpy(y).float()\n",
    "\n",
    "\n",
    "def mono_to_color(X, eps=1e-6, mean=None, std=None):\n",
    "    \"\"\"\n",
    "    Converts a one channel array to a 3 channel one in [0, 255]\n",
    "    Arguments:\n",
    "        X {numpy array [H x W]} -- 2D array to convert\n",
    "    Keyword Arguments:\n",
    "        eps {float} -- To avoid dividing by 0 (default: {1e-6})\n",
    "        mean {None or np array} -- Mean for normalization (default: {None})\n",
    "        std {None or np array} -- Std for normalization (default: {None})\n",
    "    Returns:\n",
    "        numpy array [3 x H x W] -- RGB numpy array\n",
    "    \"\"\"\n",
    "    X = np.stack([X, X, X], axis=-1)\n",
    "\n",
    "    # Standardize\n",
    "    mean = mean or X.mean()\n",
    "    std = std or X.std()\n",
    "    X = (X - mean) / (std + eps)\n",
    "\n",
    "    # Normalize to [0, 255]\n",
    "    _min, _max = X.min(), X.max()\n",
    "\n",
    "    if (_max - _min) > eps:\n",
    "        V = np.clip(X, _min, _max)\n",
    "        V = 255 * (V - _min) / (_max - _min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        V = np.zeros_like(X, dtype=np.uint8)\n",
    "\n",
    "    return V\n",
    "\n",
    "def compute_deltas(sg, width=9, order=1):\n",
    "    def librosa_delta(data, order=1, width=9):\n",
    "        return torch.from_numpy(librosa.feature.delta(data.numpy(), order=order, width=width))\n",
    "\n",
    "    if sg.shape[1] < width:\n",
    "        raise ValueError(\n",
    "            f\"\"\"Delta not possible with current settings, inputs must be wider than\n",
    "        {width} columns, try setting max_to_pad to a larger value to ensure a minimum width\"\"\"\n",
    "        )\n",
    "    new_channels = [\n",
    "        torch.stack([c, librosa_delta(c, order=1), librosa_delta(c, order=2)]) for c in sg\n",
    "    ]\n",
    "    sg.data = torch.cat(new_channels, dim=0)\n",
    "    return sg\n",
    "\n",
    "audio2mfcc = torchaudio.transforms.MFCC(n_mfcc=64, melkwargs={'n_fft':2048, 'hop_length':256, 'n_mels':128})    \n",
    "\n",
    "@torch.no_grad()\n",
    "def create_mfcc(\n",
    "        fname: str,\n",
    "        reduce_noise: bool = False,\n",
    "        frame_size: int = 5,\n",
    "        sr: int = 16000,\n",
    "        train: bool = True,\n",
    "    ) -> list:\n",
    "    waveform, sample_rate = torchaudio.load(fname)\n",
    "    # print(f\"Input: {waveform.size()}\")\n",
    "\n",
    "    # DownMixMono: convert to one channel. Some files have two channels\n",
    "    waveform = waveform.contiguous().mean(-2).unsqueeze(-2).float()\n",
    "    # print(f\"DownMixMono: {waveform.size()}\")\n",
    "\n",
    "    # Convert to 16000Hz\n",
    "    resample = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=sr)\n",
    "    waveform = resample(waveform)\n",
    "    # print(f\"Resample: {waveform.size()}\")\n",
    "\n",
    "    waveform = crop_or_pad(\n",
    "        waveform.squeeze().numpy(),\n",
    "        length=frame_size * sr,\n",
    "        sr=sr,\n",
    "        train=train\n",
    "    )\n",
    "    waveform = waveform.unsqueeze(0)\n",
    "    # print(f\"ResizeSignal: {waveform.size()}\")\n",
    "    if reduce_noise:\n",
    "        waveform = torch.tensor(nr.reduce_noise(\n",
    "            y=waveform, sr=sample_rate, win_length=256, use_tqdm=False, n_jobs=2,\n",
    "        ))\n",
    "\n",
    "    # print(f\"Before mfcc: {waveform.size()}\")\n",
    "    spectrograms = audio2mfcc(waveform)\n",
    "    # print(f\"MFCC: {spectrograms.size()}\")\n",
    "    mfcc = compute_deltas(spectrograms)\n",
    "    # print(f\"Delta: {spectrograms.size()}\")\n",
    "    mfcc = mfcc.float().div_(255.)\n",
    "    return mfcc\n",
    "\n",
    "@torch.no_grad()\n",
    "def create_mfcc_inference(\n",
    "        fname: str,\n",
    "        reduce_noise: bool = False,\n",
    "        frame_size: int = 5,\n",
    "        sr: int = 16000\n",
    "    ):\n",
    "    waveform, sample_rate = torchaudio.load(fname)\n",
    "    # print(f\"Input: {waveform.size()}\")\n",
    "\n",
    "    # DownMixMono: convert to one channel. Some files have two channels\n",
    "    waveform = waveform.contiguous().mean(-2).unsqueeze(-2).float()\n",
    "    # print(f\"DownMixMono: {waveform.size()}\")\n",
    "\n",
    "    # Convert to 16000Hz\n",
    "    resample = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=sr)\n",
    "    waveform = resample(waveform)\n",
    "    # print(f\"Resample: {waveform.size()}\")\n",
    "    \n",
    "    nb = int(frame_size * sr)\n",
    "    clip_mfccs = []\n",
    "    for i in range(ceil(waveform.size()[-1] / nb)):\n",
    "        cropped_waveform = crop_or_pad(\n",
    "            waveform.squeeze().numpy(),\n",
    "            length=frame_size * sr,\n",
    "            sr=sr,\n",
    "            train=False,\n",
    "            offset=i*sr\n",
    "        )\n",
    "        cropped_waveform = cropped_waveform.unsqueeze(0)\n",
    "        # print(f\"ResizeSignal: {cropped_waveform.size()}\")\n",
    "        if reduce_noise:\n",
    "            cropped_waveform = torch.tensor(nr.reduce_noise(\n",
    "                y=cropped_waveform, sr=sample_rate, win_length=256, use_tqdm=False, n_jobs=2,\n",
    "            ))\n",
    "        # print(f\"Before mfcc: {cropped_waveform.size()}\")\n",
    "        spectrograms = audio2mfcc(cropped_waveform)\n",
    "        # print(f\"MFCC: {spectrograms.size()}\")\n",
    "        mfcc = compute_deltas(spectrograms)\n",
    "        # print(f\"Delta: {spectrograms.size()}\")\n",
    "        mfcc = mfcc.float().div_(255.)\n",
    "        clip_mfccs.append(mfcc)\n",
    "    return torch.stack((clip_mfccs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-24T15:50:53.310093Z",
     "iopub.status.busy": "2022-02-24T15:50:53.309068Z",
     "iopub.status.idle": "2022-02-24T15:50:53.314159Z",
     "shell.execute_reply": "2022-02-24T15:50:53.314788Z",
     "shell.execute_reply.started": "2022-02-24T15:33:09.859221Z"
    },
    "papermill": {
     "duration": 4.111606,
     "end_time": "2022-02-24T15:50:53.314976",
     "exception": false,
     "start_time": "2022-02-24T15:50:49.203370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "torch.Size([3, 64, 313])\n"
     ]
    }
   ],
   "source": [
    "# Debug\n",
    "fname = Path(\"../../datasets/birdclef-2022/test_soundscapes/soundscape_453028782.ogg\")\n",
    "# fname = Path(\"../../datasets/birdclef-2022/train_audio/dunlin/XC588428.ogg\")\n",
    "spec = create_mfcc_inference(fname)\n",
    "print(len(spec))\n",
    "print(spec[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c2bf84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 available classes\n",
      "Dataset loaded with 14852 files\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class BirdCLEFDataset(Dataset):\n",
    "    def __init__(self, root_dir='/kaggle/input/birdclef-2022', is_training=False, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.is_training = is_training\n",
    "        if not is_training:\n",
    "            self.data_dir = os.path.join(self.root_dir, 'test_soundscapes')\n",
    "            self.label_file = os.path.join(self.root_dir, 'test.csv')\n",
    "            \n",
    "            # Preparing a sample submission file just to save the notebook and submit for the competition.\n",
    "            # Note : When the notebook will be submitted the below cells will be working and the genuine output file will be produced.\n",
    "            df = pd.read_csv(self.label_file)\n",
    "            test = df.copy()\n",
    "            test[\"target\"] = [False for _ in range(len(test))]\n",
    "            imp_features = [\"row_id\", \"target\"]\n",
    "            test = test[imp_features]\n",
    "            test.to_csv(\"submission.csv\", index = False)\n",
    "        else:\n",
    "            self.data_dir = os.path.join(self.root_dir, 'train_audio')\n",
    "            self.label_file = os.path.join(self.root_dir, 'train_metadata.csv')\n",
    "\n",
    "        # Retrieve categories and its label name\n",
    "        self.class_names = [\n",
    "            \"afrsil1\",\"apapan\",\"bkwpet\",\"brnowl\",\"cacgoo1\",\"chemun\",\"comwax\",\n",
    "            \"fragul\",\"grefri\",\"hawgoo\",\"iiwi\",\"layalb\",\"lotjae\",\"merlin\",\"norsho\",\n",
    "            \"parjae\",\"reccar\",\"ribgul\",\"saffin\",\"sooshe\",\"wantat1\",\"whttro\",\"akekee\",\n",
    "            \"arcter\",\"blkfra\",\"brtcur\",\"calqua\",\"chukar\",\"coopet\",\"gadwal\",\"gresca\",\n",
    "            \"hawhaw\",\"incter1\",\"lcspet\",\"madpet\",\"mitpar\",\"nutman\",\"pecsan\",\"redava\",\n",
    "            \"rinduc\",\"sander\",\"sooter1\",\"warwhe1\",\"wiltur\",\"akepa1\",\"barpet\",\"blknod\",\n",
    "            \"bubsan\",\"cangoo\",\"cintea\",\"crehon\",\"gamqua\",\"gryfra\",\"hawpet1\",\"jabwar\",\n",
    "            \"leasan\",\"magpet1\",\"moudov\",\"oahama\",\"peflov\",\"redjun\",\"rinphe\",\"semplo\",\n",
    "            \"sopsku1\",\"wesmea\",\"yebcar\",\"akiapo\",\"bcnher\",\"bongul\",\"buffle\",\"canvas\",\n",
    "            \"comgal1\",\"dunlin\",\"glwgul\",\"gwfgoo\",\"hoomer\",\"japqua\",\"leater1\",\"mallar3\",\n",
    "            \"norcar\",\"omao\",\"perfal\",\"redpha1\",\"rocpig\",\"sheowl\",\"sora\",\"wessan\",\"yefcan\",\n",
    "            \"akikik\",\"belkin1\",\"brant\",\"bulpet\",\"caster1\",\"commyn\",\"elepai\",\"gnwtea\",\n",
    "            \"hawama\",\"houfin\",\"kalphe\",\"lessca\",\"masboo\",\"norhar2\",\"osprey\",\"pibgre\",\n",
    "            \"refboo\",\"rorpar\",\"shtsan\",\"spodov\",\"wetshe\",\"zebdov\",\"amewig\",\"bkbplo\",\n",
    "            \"brnboo\",\"burpar\",\"categr\",\"compea\",\"ercfra\",\"golphe\",\"hawcoo\",\"houspa\",\n",
    "            \"kauama\",\"lesyel\",\"mauala\",\"normoc\",\"pagplo\",\"pomjae\",\"rempar\",\"rudtur\",\n",
    "            \"skylar\",\"sposan\",\"whfibi\",\"aniani\",\"bknsti\",\"brnnod\",\"buwtea\",\"chbsan\",\n",
    "            \"comsan\",\"eurwig\",\"grbher3\",\"hawcre\",\"hudgod\",\"laugul\",\"lobdow\",\"maupar\",\n",
    "            \"norpin\",\"palila\",\"puaioh\",\"rettro\",\"ruff\",\"snogoo\",\"towsol\",\"whiter\"\n",
    "        ]\n",
    "        self.num_classes = len(self.class_names)\n",
    "        print('{:d} available classes'.format(self.num_classes))\n",
    "\n",
    "        # Load image ids\n",
    "        self.sound_files = []\n",
    "        self.labels = []\n",
    "        label_df = pd.read_csv(self.label_file)\n",
    "        for index, row in label_df.iterrows():\n",
    "            label_name = row[\"primary_label\"]\n",
    "            file_name = row[\"filename\"]\n",
    "            cat_id = self.class_names.index(label_name)\n",
    "            self.sound_files.append(file_name)\n",
    "            self.labels.append(cat_id)\n",
    "\n",
    "        print('Dataset loaded with {:d} files'.format(len(self.sound_files)))\n",
    "\n",
    "        # PyTorch Transform\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sound_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Generate one sample of data\n",
    "        \"\"\"\n",
    "        filename = self.sound_files[index]\n",
    "        file_path = os.path.join(self.data_dir, filename)\n",
    "\n",
    "        # To Tensor\n",
    "        mfcc = create_mfcc(file_path)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = self.labels[index]\n",
    "\n",
    "        return mfcc, label\n",
    "\n",
    "# dataset = BirdCLEFDataset(root_dir=ROOT_DIR, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d45dbc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "class BirdCLEFDataLoader(torch.utils.data.DataLoader):\n",
    "    def __init__(self, dataset, batch_size, shuffle, validation_split=0.2, num_workers=4, collate_fn=default_collate):\n",
    "        self.validation_split = validation_split\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.batch_idx = 0\n",
    "        self.n_samples = len(dataset)\n",
    "\n",
    "        self.sampler, self.valid_sampler = self._split_sampler(self.validation_split)\n",
    "\n",
    "        self.init_kwargs = {\n",
    "            'dataset': dataset,\n",
    "            'batch_size': batch_size,\n",
    "            'shuffle': self.shuffle,\n",
    "            'collate_fn': collate_fn,\n",
    "            'num_workers': num_workers\n",
    "        }\n",
    "        super().__init__(sampler=self.sampler, **self.init_kwargs)\n",
    "\n",
    "    def _split_sampler(self, split):\n",
    "        if split == 0.0:\n",
    "            return None, None\n",
    "\n",
    "        idx_full = np.arange(self.n_samples)\n",
    "\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(idx_full)\n",
    "\n",
    "        if isinstance(split, int):\n",
    "            assert split > 0\n",
    "            assert split < self.n_samples, \"validation set size is configured to be larger than entire dataset.\"\n",
    "            len_valid = split\n",
    "        else:\n",
    "            len_valid = int(self.n_samples * split)\n",
    "\n",
    "        valid_idx = idx_full[0:len_valid]\n",
    "        train_idx = np.delete(idx_full, np.arange(0, len_valid))\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "        # turn off shuffle option which is mutually exclusive with sampler\n",
    "        self.shuffle = False\n",
    "        self.n_samples = len(train_idx)\n",
    "\n",
    "        return train_sampler, valid_sampler\n",
    "\n",
    "    def split_validation(self):\n",
    "        if self.valid_sampler is None:\n",
    "            return None\n",
    "        else:\n",
    "            return DataLoader(sampler=self.valid_sampler, **self.init_kwargs)\n",
    "\n",
    "# dataloader = BirdCLEFDataLoader(dataset, batch_size=32, shuffle=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-24T15:53:17.895684Z",
     "iopub.status.busy": "2022-02-24T15:53:17.881397Z",
     "iopub.status.idle": "2022-02-24T15:56:31.382406Z",
     "shell.execute_reply": "2022-02-24T15:56:31.382861Z",
     "shell.execute_reply.started": "2022-02-24T15:36:03.081657Z"
    },
    "papermill": {
     "duration": 197.529644,
     "end_time": "2022-02-24T15:56:31.383059",
     "exception": false,
     "start_time": "2022-02-24T15:53:13.853415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test files: 1\n"
     ]
    }
   ],
   "source": [
    "TEST_DIR = ROOT_DIR / \"test_soundscapes\"\n",
    "test_files = sorted(TEST_DIR.glob(\"**/*.ogg\"))\n",
    "print(f\"Total test files: {len(test_files)}\")\n",
    "\n",
    "class_names = [\n",
    "    \"afrsil1\",\"apapan\",\"bkwpet\",\"brnowl\",\"cacgoo1\",\"chemun\",\"comwax\",\n",
    "    \"fragul\",\"grefri\",\"hawgoo\",\"iiwi\",\"layalb\",\"lotjae\",\"merlin\",\"norsho\",\n",
    "    \"parjae\",\"reccar\",\"ribgul\",\"saffin\",\"sooshe\",\"wantat1\",\"whttro\",\"akekee\",\n",
    "    \"arcter\",\"blkfra\",\"brtcur\",\"calqua\",\"chukar\",\"coopet\",\"gadwal\",\"gresca\",\n",
    "    \"hawhaw\",\"incter1\",\"lcspet\",\"madpet\",\"mitpar\",\"nutman\",\"pecsan\",\"redava\",\n",
    "    \"rinduc\",\"sander\",\"sooter1\",\"warwhe1\",\"wiltur\",\"akepa1\",\"barpet\",\"blknod\",\n",
    "    \"bubsan\",\"cangoo\",\"cintea\",\"crehon\",\"gamqua\",\"gryfra\",\"hawpet1\",\"jabwar\",\n",
    "    \"leasan\",\"magpet1\",\"moudov\",\"oahama\",\"peflov\",\"redjun\",\"rinphe\",\"semplo\",\n",
    "    \"sopsku1\",\"wesmea\",\"yebcar\",\"akiapo\",\"bcnher\",\"bongul\",\"buffle\",\"canvas\",\n",
    "    \"comgal1\",\"dunlin\",\"glwgul\",\"gwfgoo\",\"hoomer\",\"japqua\",\"leater1\",\"mallar3\",\n",
    "    \"norcar\",\"omao\",\"perfal\",\"redpha1\",\"rocpig\",\"sheowl\",\"sora\",\"wessan\",\"yefcan\",\n",
    "    \"akikik\",\"belkin1\",\"brant\",\"bulpet\",\"caster1\",\"commyn\",\"elepai\",\"gnwtea\",\n",
    "    \"hawama\",\"houfin\",\"kalphe\",\"lessca\",\"masboo\",\"norhar2\",\"osprey\",\"pibgre\",\n",
    "    \"refboo\",\"rorpar\",\"shtsan\",\"spodov\",\"wetshe\",\"zebdov\",\"amewig\",\"bkbplo\",\n",
    "    \"brnboo\",\"burpar\",\"categr\",\"compea\",\"ercfra\",\"golphe\",\"hawcoo\",\"houspa\",\n",
    "    \"kauama\",\"lesyel\",\"mauala\",\"normoc\",\"pagplo\",\"pomjae\",\"rempar\",\"rudtur\",\n",
    "    \"skylar\",\"sposan\",\"whfibi\",\"aniani\",\"bknsti\",\"brnnod\",\"buwtea\",\"chbsan\",\n",
    "    \"comsan\",\"eurwig\",\"grbher3\",\"hawcre\",\"hudgod\",\"laugul\",\"lobdow\",\"maupar\",\n",
    "    \"norpin\",\"palila\",\"puaioh\",\"rettro\",\"ruff\",\"snogoo\",\"towsol\",\"whiter\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('models/xresnet34-stage1-before-unfreeze-68.pth', map_location=torch.device('cpu'))\n",
    "# model = xresnet34(n_out=len(class_names))\n",
    "model = xresnet34()\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 3, 64, 313])\n",
      "torch.Size([12, 1000])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    for fname in tqdm(test_files):\n",
    "        batch = create_mfcc_inference(\n",
    "            fname,\n",
    "            reduce_noise=True,\n",
    "            frame_size=5,\n",
    "            sr=16000\n",
    "        )\n",
    "        print(batch.size())\n",
    "        batch = batch.to(device)\n",
    "        preds = model(batch)\n",
    "        print(preds.size())\n",
    "    predictions += preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 4.073081,
     "end_time": "2022-02-24T20:34:16.842034",
     "exception": false,
     "start_time": "2022-02-24T20:34:12.768953",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-------------------------------------------\n",
    "# Format Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_DATASET, \"scored_birds.json\")) as fp:\n",
    "    scored_birds = json.load(fp)\n",
    "\n",
    "print(scored_birds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = []\n",
    "for i, row in tqdm(df_converted.iterrows(), total=len(df_converted)):\n",
    "    preds = dict(zip(model.labels, predictions[i]))\n",
    "    for bird in scored_birds:\n",
    "        submission.append({\n",
    "            \"row_id\": f\"{row['file_id']}_{bird}_{row['end_time']}\",\n",
    "            \"target\": preds[bird] > 1. / len(scored_birds),\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame(submission).set_index(\"row_id\")\n",
    "df_submission.to_csv(\"submission.csv\")\n",
    "\n",
    "! head submission.csv"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4ea5d56035456c2b1b6b90ed23f43434caf1e4d5f44691eca1785db1c6f7643"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17996.244563,
   "end_time": "2022-02-24T20:43:16.601349",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-24T15:43:20.356786",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "123f7b325eb449c8b13e4fd4af8bf369": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3a683348cce645ff98ddf3c3e188cc48": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8fef9b0d05ff4f7a9731bee2222367b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "Processing: ",
       "description_tooltip": null,
       "layout": "IPY_MODEL_123f7b325eb449c8b13e4fd4af8bf369",
       "max": 3,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d95b1d1e05f6413e86a2cd17ad6a9e55",
       "value": 3
      }
     },
     "a8e4a560d94740f2a516322b0520e1bf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "Processing: ",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ae6c2b5a5392426991c5fe17320163d5",
       "max": 5,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3a683348cce645ff98ddf3c3e188cc48",
       "value": 5
      }
     },
     "ae6c2b5a5392426991c5fe17320163d5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d95b1d1e05f6413e86a2cd17ad6a9e55": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
