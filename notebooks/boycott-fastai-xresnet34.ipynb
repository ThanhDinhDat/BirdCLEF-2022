{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import noisereduce as nr\n",
    "\n",
    "import numpy as np\n",
    "# import modin.pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "torchaudio.set_audio_backend(\"sox_io\")\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATIONS\n",
    "IS_KAGGLE = False\n",
    "if IS_KAGGLE:\n",
    "    ROOT_DIR = Path('../../input/birdclef-2022')\n",
    "else:\n",
    "    ROOT_DIR = Path('../../datasets/birdclef-2022')\n",
    "DATA_DIR = ROOT_DIR / \"train_audio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "\n",
    "def crop_or_pad(y, length, sr, train=True, offset=0, probs=None):\n",
    "    \"\"\"\n",
    "    Crops an array to a chosen length\n",
    "    Arguments:\n",
    "        y {1D np array} -- Array to crop\n",
    "        length {int} -- Length of the crop\n",
    "        sr {int} -- Sampling rate\n",
    "    Keyword Arguments:\n",
    "        train {bool} -- Whether we are at train time. If so, crop randomly, else return the beginning of y (default: {True})\n",
    "        probs {None or numpy array} -- Probabilities to use to chose where to crop (default: {None})\n",
    "    Returns:\n",
    "        1D np array -- Cropped array\n",
    "    \"\"\"\n",
    "    if len(y) <= length:\n",
    "        y = np.concatenate([y, np.zeros(length - len(y))])\n",
    "    else:\n",
    "        if not train:\n",
    "            start = 0 + offset\n",
    "        elif probs is None:\n",
    "            start = np.random.randint(len(y) - length)\n",
    "        else:\n",
    "            start = (\n",
    "                    np.random.choice(np.arange(len(probs)), p=probs) + np.random.random()\n",
    "            )\n",
    "            start = int(sr * (start))\n",
    "\n",
    "        y = y[start: start + length]\n",
    "\n",
    "    return torch.from_numpy(y).float()\n",
    "\n",
    "\n",
    "def mono_to_color(X, eps=1e-6, mean=None, std=None):\n",
    "    \"\"\"\n",
    "    Converts a one channel array to a 3 channel one in [0, 255]\n",
    "    Arguments:\n",
    "        X {numpy array [H x W]} -- 2D array to convert\n",
    "    Keyword Arguments:\n",
    "        eps {float} -- To avoid dividing by 0 (default: {1e-6})\n",
    "        mean {None or np array} -- Mean for normalization (default: {None})\n",
    "        std {None or np array} -- Std for normalization (default: {None})\n",
    "    Returns:\n",
    "        numpy array [3 x H x W] -- RGB numpy array\n",
    "    \"\"\"\n",
    "    X = np.stack([X, X, X], axis=-1)\n",
    "\n",
    "    # Standardize\n",
    "    mean = mean or X.mean()\n",
    "    std = std or X.std()\n",
    "    X = (X - mean) / (std + eps)\n",
    "\n",
    "    # Normalize to [0, 255]\n",
    "    _min, _max = X.min(), X.max()\n",
    "\n",
    "    if (_max - _min) > eps:\n",
    "        V = np.clip(X, _min, _max)\n",
    "        V = 255 * (V - _min) / (_max - _min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        V = np.zeros_like(X, dtype=np.uint8)\n",
    "\n",
    "    return V\n",
    "\n",
    "def compute_deltas(sg, width=9, order=1):\n",
    "    def librosa_delta(data, order=1, width=9):\n",
    "        return torch.from_numpy(librosa.feature.delta(data.numpy(), order=order, width=width))\n",
    "\n",
    "    if sg.shape[1] < width:\n",
    "        raise ValueError(\n",
    "            f\"\"\"Delta not possible with current settings, inputs must be wider than\n",
    "        {width} columns, try setting max_to_pad to a larger value to ensure a minimum width\"\"\"\n",
    "        )\n",
    "    new_channels = [\n",
    "        torch.stack([c, librosa_delta(c, order=1), librosa_delta(c, order=2)]) for c in sg\n",
    "    ]\n",
    "    sg.data = torch.cat(new_channels, dim=0)\n",
    "    return sg\n",
    "\n",
    "audio2mfcc = torchaudio.transforms.MFCC(n_mfcc=64, melkwargs={'n_fft':2048, 'hop_length':256, 'n_mels':128})    \n",
    "\n",
    "@torch.no_grad()\n",
    "def create_mfcc(\n",
    "        fname: str,\n",
    "        reduce_noise: bool = False,\n",
    "        frame_size: int = 5,\n",
    "        sr: int = 16000,\n",
    "        train: bool = True,\n",
    "    ) -> list:\n",
    "    waveform, sample_rate = torchaudio.load(fname)\n",
    "    # print(f\"Input: {waveform.size()}\")\n",
    "\n",
    "    # DownMixMono: convert to one channel. Some files have two channels\n",
    "    waveform = waveform.contiguous().mean(-2).unsqueeze(-2).float()\n",
    "    # print(f\"DownMixMono: {waveform.size()}\")\n",
    "\n",
    "    # Convert to 16000Hz\n",
    "    resample = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=sr)\n",
    "    waveform = resample(waveform)\n",
    "    # print(f\"Resample: {waveform.size()}\")\n",
    "\n",
    "    waveform = crop_or_pad(\n",
    "        waveform.squeeze().numpy(),\n",
    "        length=frame_size * sr,\n",
    "        sr=sr,\n",
    "        train=train\n",
    "    )\n",
    "    waveform = waveform.unsqueeze(0)\n",
    "    # print(f\"ResizeSignal: {waveform.size()}\")\n",
    "    if reduce_noise:\n",
    "        waveform = torch.tensor(nr.reduce_noise(\n",
    "            y=waveform, sr=sample_rate, win_length=256, use_tqdm=False, n_jobs=2,\n",
    "        ))\n",
    "\n",
    "    # print(f\"Before mfcc: {waveform.size()}\")\n",
    "    spectrograms = audio2mfcc(waveform)\n",
    "    # print(f\"MFCC: {spectrograms.size()}\")\n",
    "    mfcc = compute_deltas(spectrograms)\n",
    "    # print(f\"Delta: {spectrograms.size()}\")\n",
    "    mfcc = mfcc.float().div_(255.)\n",
    "    return mfcc\n",
    "\n",
    "@torch.no_grad()\n",
    "def create_mfcc_inference(\n",
    "        fname: str,\n",
    "        reduce_noise: bool = False,\n",
    "        frame_size: int = 5,\n",
    "        sr: int = 16000\n",
    "    ):\n",
    "    waveform, sample_rate = torchaudio.load(fname)\n",
    "    # print(f\"Input: {waveform.size()}\")\n",
    "\n",
    "    # DownMixMono: convert to one channel. Some files have two channels\n",
    "    waveform = waveform.contiguous().mean(-2).unsqueeze(-2).float()\n",
    "    # print(f\"DownMixMono: {waveform.size()}\")\n",
    "\n",
    "    # Convert to 16000Hz\n",
    "    resample = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=sr)\n",
    "    waveform = resample(waveform)\n",
    "    # print(f\"Resample: {waveform.size()}\")\n",
    "    \n",
    "    nb = int(frame_size * sr)\n",
    "    clip_mfccs = []\n",
    "    for i in range(ceil(waveform.size()[-1] / nb)):\n",
    "        cropped_waveform = crop_or_pad(\n",
    "            waveform.squeeze().numpy(),\n",
    "            length=frame_size * sr,\n",
    "            sr=sr,\n",
    "            train=False,\n",
    "            offset=i*sr\n",
    "        )\n",
    "        cropped_waveform = cropped_waveform.unsqueeze(0)\n",
    "        # print(f\"ResizeSignal: {cropped_waveform.size()}\")\n",
    "        if reduce_noise:\n",
    "            cropped_waveform = torch.tensor(nr.reduce_noise(\n",
    "                y=cropped_waveform, sr=sample_rate, win_length=256, use_tqdm=False, n_jobs=2,\n",
    "            ))\n",
    "        # print(f\"Before mfcc: {cropped_waveform.size()}\")\n",
    "        spectrograms = audio2mfcc(cropped_waveform)\n",
    "        # print(f\"MFCC: {spectrograms.size()}\")\n",
    "        mfcc = compute_deltas(spectrograms)\n",
    "        # print(f\"Delta: {spectrograms.size()}\")\n",
    "        # mfcc = mfcc.float().div_(255.)\n",
    "        clip_mfccs.append(mfcc)\n",
    "    return torch.stack((clip_mfccs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "torch.Size([3, 64, 313])\n"
     ]
    }
   ],
   "source": [
    "# Debug\n",
    "fname = Path(\"../../datasets/birdclef-2022/test_soundscapes/soundscape_453028782.ogg\")\n",
    "# fname = Path(\"../../datasets/birdclef-2022/train_audio/dunlin/XC588428.ogg\")\n",
    "spec = create_mfcc_inference(fname)\n",
    "print(len(spec))\n",
    "print(spec[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 available classes\n",
      "Dataset loaded with 14852 files\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class BirdCLEFDataset(Dataset):\n",
    "    def __init__(self, root_dir='/kaggle/input/birdclef-2022', is_training=False, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.is_training = is_training\n",
    "        if not is_training:\n",
    "            self.data_dir = os.path.join(self.root_dir, 'test_soundscapes')\n",
    "            self.label_file = os.path.join(self.root_dir, 'test.csv')\n",
    "            \n",
    "            # Preparing a sample submission file just to save the notebook and submit for the competition.\n",
    "            # Note : When the notebook will be submitted the below cells will be working and the genuine output file will be produced.\n",
    "            df = pd.read_csv(self.label_file)\n",
    "            test = df.copy()\n",
    "            test[\"target\"] = [False for _ in range(len(test))]\n",
    "            imp_features = [\"row_id\", \"target\"]\n",
    "            test = test[imp_features]\n",
    "            test.to_csv(\"submission.csv\", index = False)\n",
    "        else:\n",
    "            self.data_dir = os.path.join(self.root_dir, 'train_audio')\n",
    "            self.label_file = os.path.join(self.root_dir, 'train_metadata.csv')\n",
    "\n",
    "        # Retrieve categories and its label name\n",
    "        self.class_names = [\n",
    "            \"afrsil1\",\"apapan\",\"bkwpet\",\"brnowl\",\"cacgoo1\",\"chemun\",\"comwax\",\n",
    "            \"fragul\",\"grefri\",\"hawgoo\",\"iiwi\",\"layalb\",\"lotjae\",\"merlin\",\"norsho\",\n",
    "            \"parjae\",\"reccar\",\"ribgul\",\"saffin\",\"sooshe\",\"wantat1\",\"whttro\",\"akekee\",\n",
    "            \"arcter\",\"blkfra\",\"brtcur\",\"calqua\",\"chukar\",\"coopet\",\"gadwal\",\"gresca\",\n",
    "            \"hawhaw\",\"incter1\",\"lcspet\",\"madpet\",\"mitpar\",\"nutman\",\"pecsan\",\"redava\",\n",
    "            \"rinduc\",\"sander\",\"sooter1\",\"warwhe1\",\"wiltur\",\"akepa1\",\"barpet\",\"blknod\",\n",
    "            \"bubsan\",\"cangoo\",\"cintea\",\"crehon\",\"gamqua\",\"gryfra\",\"hawpet1\",\"jabwar\",\n",
    "            \"leasan\",\"magpet1\",\"moudov\",\"oahama\",\"peflov\",\"redjun\",\"rinphe\",\"semplo\",\n",
    "            \"sopsku1\",\"wesmea\",\"yebcar\",\"akiapo\",\"bcnher\",\"bongul\",\"buffle\",\"canvas\",\n",
    "            \"comgal1\",\"dunlin\",\"glwgul\",\"gwfgoo\",\"hoomer\",\"japqua\",\"leater1\",\"mallar3\",\n",
    "            \"norcar\",\"omao\",\"perfal\",\"redpha1\",\"rocpig\",\"sheowl\",\"sora\",\"wessan\",\"yefcan\",\n",
    "            \"akikik\",\"belkin1\",\"brant\",\"bulpet\",\"caster1\",\"commyn\",\"elepai\",\"gnwtea\",\n",
    "            \"hawama\",\"houfin\",\"kalphe\",\"lessca\",\"masboo\",\"norhar2\",\"osprey\",\"pibgre\",\n",
    "            \"refboo\",\"rorpar\",\"shtsan\",\"spodov\",\"wetshe\",\"zebdov\",\"amewig\",\"bkbplo\",\n",
    "            \"brnboo\",\"burpar\",\"categr\",\"compea\",\"ercfra\",\"golphe\",\"hawcoo\",\"houspa\",\n",
    "            \"kauama\",\"lesyel\",\"mauala\",\"normoc\",\"pagplo\",\"pomjae\",\"rempar\",\"rudtur\",\n",
    "            \"skylar\",\"sposan\",\"whfibi\",\"aniani\",\"bknsti\",\"brnnod\",\"buwtea\",\"chbsan\",\n",
    "            \"comsan\",\"eurwig\",\"grbher3\",\"hawcre\",\"hudgod\",\"laugul\",\"lobdow\",\"maupar\",\n",
    "            \"norpin\",\"palila\",\"puaioh\",\"rettro\",\"ruff\",\"snogoo\",\"towsol\",\"whiter\"\n",
    "        ]\n",
    "        self.num_classes = len(self.class_names)\n",
    "        print('{:d} available classes'.format(self.num_classes))\n",
    "\n",
    "        # Load image ids\n",
    "        self.sound_files = []\n",
    "        self.labels = []\n",
    "        label_df = pd.read_csv(self.label_file)\n",
    "        for index, row in label_df.iterrows():\n",
    "            label_name = row[\"primary_label\"]\n",
    "            file_name = row[\"filename\"]\n",
    "            cat_id = self.class_names.index(label_name)\n",
    "            self.sound_files.append(file_name)\n",
    "            self.labels.append(cat_id)\n",
    "\n",
    "        print('Dataset loaded with {:d} files'.format(len(self.sound_files)))\n",
    "\n",
    "        # PyTorch Transform\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sound_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Generate one sample of data\n",
    "        \"\"\"\n",
    "        filename = self.sound_files[index]\n",
    "        file_path = os.path.join(self.data_dir, filename)\n",
    "\n",
    "        # To Tensor\n",
    "        mfcc = create_mfcc(file_path)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = self.labels[index]\n",
    "\n",
    "        return mfcc, label\n",
    "\n",
    "dataset = BirdCLEFDataset(root_dir=ROOT_DIR, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "class BirdCLEFDataLoader(torch.utils.data.DataLoader):\n",
    "    def __init__(self, dataset, batch_size, shuffle, validation_split=0.2, num_workers=4, collate_fn=default_collate):\n",
    "        self.validation_split = validation_split\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.batch_idx = 0\n",
    "        self.n_samples = len(dataset)\n",
    "\n",
    "        self.sampler, self.valid_sampler = self._split_sampler(self.validation_split)\n",
    "\n",
    "        self.init_kwargs = {\n",
    "            'dataset': dataset,\n",
    "            'batch_size': batch_size,\n",
    "            'shuffle': self.shuffle,\n",
    "            'collate_fn': collate_fn,\n",
    "            'num_workers': num_workers\n",
    "        }\n",
    "        super().__init__(sampler=self.sampler, **self.init_kwargs)\n",
    "\n",
    "    def _split_sampler(self, split):\n",
    "        if split == 0.0:\n",
    "            return None, None\n",
    "\n",
    "        idx_full = np.arange(self.n_samples)\n",
    "\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(idx_full)\n",
    "\n",
    "        if isinstance(split, int):\n",
    "            assert split > 0\n",
    "            assert split < self.n_samples, \"validation set size is configured to be larger than entire dataset.\"\n",
    "            len_valid = split\n",
    "        else:\n",
    "            len_valid = int(self.n_samples * split)\n",
    "\n",
    "        valid_idx = idx_full[0:len_valid]\n",
    "        train_idx = np.delete(idx_full, np.arange(0, len_valid))\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "        # turn off shuffle option which is mutually exclusive with sampler\n",
    "        self.shuffle = False\n",
    "        self.n_samples = len(train_idx)\n",
    "\n",
    "        return train_sampler, valid_sampler\n",
    "\n",
    "    def split_validation(self):\n",
    "        if self.valid_sampler is None:\n",
    "            return None\n",
    "        else:\n",
    "            return torch.utils.data.DataLoader(sampler=self.valid_sampler, **self.init_kwargs)\n",
    "\n",
    "data_loader = BirdCLEFDataLoader(dataset, batch_size=32, shuffle=True, num_workers=8)\n",
    "val_data_loader = data_loader.split_validation()\n",
    "print(len(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    \"afrsil1\",\"apapan\",\"bkwpet\",\"brnowl\",\"cacgoo1\",\"chemun\",\"comwax\",\n",
    "    \"fragul\",\"grefri\",\"hawgoo\",\"iiwi\",\"layalb\",\"lotjae\",\"merlin\",\"norsho\",\n",
    "    \"parjae\",\"reccar\",\"ribgul\",\"saffin\",\"sooshe\",\"wantat1\",\"whttro\",\"akekee\",\n",
    "    \"arcter\",\"blkfra\",\"brtcur\",\"calqua\",\"chukar\",\"coopet\",\"gadwal\",\"gresca\",\n",
    "    \"hawhaw\",\"incter1\",\"lcspet\",\"madpet\",\"mitpar\",\"nutman\",\"pecsan\",\"redava\",\n",
    "    \"rinduc\",\"sander\",\"sooter1\",\"warwhe1\",\"wiltur\",\"akepa1\",\"barpet\",\"blknod\",\n",
    "    \"bubsan\",\"cangoo\",\"cintea\",\"crehon\",\"gamqua\",\"gryfra\",\"hawpet1\",\"jabwar\",\n",
    "    \"leasan\",\"magpet1\",\"moudov\",\"oahama\",\"peflov\",\"redjun\",\"rinphe\",\"semplo\",\n",
    "    \"sopsku1\",\"wesmea\",\"yebcar\",\"akiapo\",\"bcnher\",\"bongul\",\"buffle\",\"canvas\",\n",
    "    \"comgal1\",\"dunlin\",\"glwgul\",\"gwfgoo\",\"hoomer\",\"japqua\",\"leater1\",\"mallar3\",\n",
    "    \"norcar\",\"omao\",\"perfal\",\"redpha1\",\"rocpig\",\"sheowl\",\"sora\",\"wessan\",\"yefcan\",\n",
    "    \"akikik\",\"belkin1\",\"brant\",\"bulpet\",\"caster1\",\"commyn\",\"elepai\",\"gnwtea\",\n",
    "    \"hawama\",\"houfin\",\"kalphe\",\"lessca\",\"masboo\",\"norhar2\",\"osprey\",\"pibgre\",\n",
    "    \"refboo\",\"rorpar\",\"shtsan\",\"spodov\",\"wetshe\",\"zebdov\",\"amewig\",\"bkbplo\",\n",
    "    \"brnboo\",\"burpar\",\"categr\",\"compea\",\"ercfra\",\"golphe\",\"hawcoo\",\"houspa\",\n",
    "    \"kauama\",\"lesyel\",\"mauala\",\"normoc\",\"pagplo\",\"pomjae\",\"rempar\",\"rudtur\",\n",
    "    \"skylar\",\"sposan\",\"whfibi\",\"aniani\",\"bknsti\",\"brnnod\",\"buwtea\",\"chbsan\",\n",
    "    \"comsan\",\"eurwig\",\"grbher3\",\"hawcre\",\"hudgod\",\"laugul\",\"lobdow\",\"maupar\",\n",
    "    \"norpin\",\"palila\",\"puaioh\",\"rettro\",\"ruff\",\"snogoo\",\"towsol\",\"whiter\"\n",
    "]\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "hparams = {\n",
    "    \"lr\": 3e-3,\n",
    "    \"epochs\": 20,\n",
    "    \"batch_size\": 32,\n",
    "    \"data_len\": len(data_loader)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn import functional as F\n",
    "import torchmetrics\n",
    "\n",
    "class BirdCLEFModel(pl.LightningModule):\n",
    "    def __init__(self, backbone, input_shape, num_classes, loss_fn, learning_rate=2e-4):\n",
    "        super().__init__()\n",
    "\n",
    "        # log hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dim = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Loss function\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        n_sizes = backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Sequential()\n",
    "        # # layers are frozen by using eval()\n",
    "        # self.backbone.eval()\n",
    "        # # freeze params\n",
    "        # for param in self.backbone.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        self.backbone.fc = nn.Linear(n_sizes, num_classes)\n",
    "        \n",
    "        # n_sizes = self._get_conv_output(input_shape)\n",
    "        # self.classifier = nn.Linear(n_sizes, num_classes)\n",
    "\n",
    "    # returns the size of the output tensor going into the Linear layer from the conv block.\n",
    "    def _get_conv_output(self, shape):\n",
    "        batch_size = 1\n",
    "        tmp_input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
    "\n",
    "        output_feat = self._forward_features(tmp_input) \n",
    "        n_size = output_feat.data.view(batch_size, -1).size(1)\n",
    "        return n_size\n",
    "        \n",
    "    # returns the feature tensor from the conv block\n",
    "    def _forward_features(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "    \n",
    "    # will be used during inference\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # x = F.log_softmax(self.classifier(x), dim=1)\n",
    "        # x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        # loss = F.cross_entropy(y_hat, y)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        self.accuracy(y_hat, y)\n",
    "        self.log('train_acc_step', self.accuracy.compute())\n",
    "        return loss\n",
    "    \n",
    "    def training_epoch_end(self, outs):\n",
    "        # log epoch metric\n",
    "        self.log('train_acc_epoch', self.accuracy.compute())\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        # loss = F.cross_entropy(y_hat, y)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('valid_loss', loss, on_step=True)\n",
    "        self.accuracy(y_hat, y)\n",
    "        self.log('val_acc_step', self.accuracy.compute())\n",
    "    \n",
    "    def validation_epoch_end(self, outs):\n",
    "        # log epoch metric\n",
    "        self.log('val_acc_epoch', self.accuracy.compute())\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        # loss = F.cross_entropy(y_hat, y)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "        self.accuracy(y_hat, y)\n",
    "        self.log('test_acc_step', self.accuracy.compute())\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=self.learning_rate, momentum=0.9)\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            self.learning_rate,\n",
    "            steps_per_epoch=hparams[\"data_len\"],\n",
    "            epochs=hparams[\"epochs\"]\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\" : \"step\" }\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell\n",
    "# class LabelSmoothingCrossEntropy(torch.nn.Module):\n",
    "#     y_int = True\n",
    "#     def __init__(self, eps:float=0.1, weight=None, reduction='mean'):\n",
    "#         self.eps = eps\n",
    "#         self.weight = weight\n",
    "#         self.reduction = reduction\n",
    "\n",
    "#     def forward(self, output, target):\n",
    "#         c = output.size()[1]\n",
    "#         log_preds = F.log_softmax(output, dim=1)\n",
    "#         if self.reduction=='sum': loss = -log_preds.sum()\n",
    "#         else:\n",
    "#             loss = -log_preds.sum(dim=1) #We divide by that size at the return line so sum and not mean\n",
    "#             if self.reduction=='mean':  loss = loss.mean()\n",
    "#         return loss*self.eps/c + (1-self.eps) * F.nll_loss(log_preds, target.long(), weight=self.weight, reduction=self.reduction)\n",
    "\n",
    "#     # def activation(self, out): return F.softmax(out, dim=-1)\n",
    "#     # def decodes(self, out):    return out.argmax(dim=-1)\n",
    "\n",
    "def label_smoothing_cross_entropy_loss(output, target, eps=0.1, weight=None, reduction='mean'):\n",
    "        c = output.size()[1]\n",
    "        log_preds = F.log_softmax(output, dim=1)\n",
    "        if reduction=='sum': loss = -log_preds.sum()\n",
    "        else:\n",
    "            loss = -log_preds.sum(dim=1) #We divide by that size at the return line so sum and not mean\n",
    "            if reduction=='mean':  loss = loss.mean()\n",
    "        return loss*eps/c + (1-eps) * F.nll_loss(log_preds, target.long(), weight=weight, reduction=reduction)\n",
    "\n",
    "# from fastai.losses import LabelSmoothingCrossEntropy\n",
    "# loss_function = torch.nn.CrossEntropyLoss()\n",
    "# loss_function = loss_function.to(device)\n",
    "loss_function = label_smoothing_cross_entropy_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "backbone = models.resnet34(pretrained=True)\n",
    "model = BirdCLEFModel(\n",
    "    backbone=backbone,\n",
    "    input_shape=(3, 64, 313),\n",
    "    num_classes=len(class_names),\n",
    "    loss_fn=loss_function,\n",
    "    learning_rate=hparams[\"lr\"]\n",
    ")\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:51<00:00,  2.56it/s]Restoring states from the checkpoint path at /home/kitemetric/workspace/kaggle-challenges/BirdCLEF-2022/notebooks/lr_find_temp_model_7d9448de-55ef-473b-ae13-832e6f8d4bba.ckpt\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:56<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested LR: 0.002754228703338169\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAoOUlEQVR4nO3dd3xc1Z338c9PvRfbkmzJTXLHNBs3wJhebCAQUmAJBAjEISHJkmTzsOyy2ZBkd/M8pJAsBHAIEDqE0EnAdINplivuRW6SbEu2eh/NnOePGTmSLNuSrdGMZ77v10svzcy9mvvzRcxX59xzzzHnHCIiEr1iQl2AiIiEloJARCTKKQhERKKcgkBEJMopCEREopyCQEQkysUF883NLAt4EDgecMA3nHMfd9puwO+AeUATcL1zbtmh3nPIkCFu9OjRwSpZRCQiLV26dK9zLqenbUENAvwf8q87575sZglASrftc4Fxga+ZwH2B7wc1evRoiouLg1GriEjEMrPtB9sWtK4hM8sE5gB/AnDOtTnnarrtdhnwqPP7BMgys2HBqklERA4UzGsEhUAl8LCZLTezB80stds+BcDOTs9LA6+JiMgACWYQxAFTgfucc1OARuBfj+SNzGy+mRWbWXFlZWV/1igiEvWCGQSlQKlz7tPA8+fwB0NnZcCITs+HB17rwjm3wDk3zTk3LSenx2sdIiJyhIIWBM653cBOM5sQeOlcYG233V4Gvm5+s4Ba59yuYNUkIiIHCvaooe8BTwRGDJUAN5jZzQDOufuBv+EfOroZ//DRG4Jcj4iIdBPUIHDOrQCmdXv5/k7bHXBLMGsQEYkEC9fsZlxeOoVDuo+5OXq6s1hEJMy1e33c8uQynlmy8/A7HwEFgYhImNte1YTH6xiXmxaU91cQiIiEuU17GgAYqyAQEYlOmyvqARijIBARiU6bKxrIz0wiLTE443sUBCIiYW5TRQNj89KD9v4KAhGRMObzObZUNgTtQjEoCEREwlpZTTMtHl/QLhSDgkBEJKxtrvCPGFKLQEQkSm0KjBhSi0BEJEpt2tPAkLREslISgnYMBYGISBjbXNnA2Nz+n1+oMwWBiEiYcs6xeU8D43KDN3QUFAQiImGror6V+tb2oF4fAAWBiEjY6phjKJgjhkBBICIStjYPwIghUBCIiIStTRUNZCTFkZOeGNTjKAhERMLU5ooGxuamYWZBPY6CQEQkTG2uCP6IIVAQiIiEperGNvY1tjEmyPcQgIJARCQsbakM7qpknSkIRETCUEllIwBjchQEIiJRaUtlAwmxMQzPTgn6sRQEIiJhaEtlA6OHpBAbE9wRQ6AgEBEJS1sqGwekWwgUBCIiYaet3ceOqiYFgYhItNpR1YjX5wZk6CgoCEREws7mioEbMQQKAhGRsNNxD0HhELUIRESi0pbKBvIyEklPih+Q4ykIRETCzECOGAIFgYhIWHHOUVLZoCAQEYlWlQ2t1Le0MyZnYK4PgIJARCSsbAmMGCpSi0BEJDp1jBgaMwCzjnZQEIiIhJEtlQ0kx8cyLCNpwI4ZF8w3N7NtQD3gBdqdc9O6bc8EHgdGBmr5lXPu4WDWJCISzkoqGynKSSVmACab6xDUIAg42zm39yDbbgHWOucuNbMcYIOZPeGcaxuAukREws6Wygamjswe0GOGumvIAenmX5k5DagC2kNbkohIaNS1eCiraaZoAEcMQfCDwAELzWypmc3vYfs9wCSgHPgc+GfnnC/INYmIhKVXV+7COThzfM6AHjfYQTDbOTcVmAvcYmZzum2/EFgB5AMnA/eYWUb3NzGz+WZWbGbFlZWVQS5ZRCQ0nineyfi8NE4ekTWgxw1qEDjnygLfK4AXgBnddrkBeN75bQa2AhN7eJ8FzrlpzrlpOTkDm5QiIgNhw+56Vu6s4avTRuDvLR84QQsCM0s1s/SOx8AFwOpuu+0Azg3skwdMAEqCVZOISLh6tngn8bHGF6cUDPixgzlqKA94IZBsccCTzrnXzexmAOfc/cDPgUfM7HPAgNsOMcJIRCQitbX7eGF5GedNymNwWuKAHz9oQeCcKwFO6uH1+zs9LsffUhARiVpvr9tDVWMbX50+IiTHD/XwURGRqPdM8U6GZSYxZ1xoroEqCEREQqi22cOijZVcMbWA2AG8m7gzBYGISAjtbWjF52BcbnrIalAQiIiEUH2LfzKF9KSBmPGnZwoCEZEQqm/xAAzY+sQ9URCIiIRQg1oEIiLRTV1DIiJRrq6jayhRXUMiIlGpo0WQphaBiEh0qm9pJzUhNmT3EICCQEQkpBpaPSEdMQQKAhGRkKpvaQ/phWJQEIiIhFR9S3tIrw+AgkBEJKTqW9Q1JCIS1dQ1JCIS5epb28lQEIiIRC91DYmIRDGP10eLx0daoloEIiJRKRzmGQIFgYhIyITDFNSgIBARCRm1CEREopyCQEQkytWHwRTUoCAQEQkZtQhERKJcQ6uCQEQkqmnUkIhIlKtvaScxLoaEuNB+FCsIRERCpC4MJpwDBYGISMiEwzxDoCAQEQmZhla1CEREolo4rEUACgIRkZCpb/GE/GYyUBCIiIRMOKxXDAoCEZGQUdeQiEgU8/lc4GKxuoZERKJSQ5t/eolQr1cMCgIRkZAIlwnnAIJagZltA+oBL9DunJvWwz5nAXcD8cBe59yZwaxJRCQcdMwzlBYGo4YGIorOds7t7WmDmWUBfwAucs7tMLPcAahHRCTkwqlFEOquoauB551zOwCccxUhrkdEZEA0RFEQOGChmS01s/k9bB8PZJvZe4F9vt7Tm5jZfDMrNrPiysrKoBYsIjIQ6sJkCmoIftfQbOdcWaDL500zW++cW9Tt+KcA5wLJwMdm9olzbmPnN3HOLQAWAEybNs0FuWYRkaDr6BqK+FFDzrmywPcK4AVgRrddSoE3nHONgesIi4CTglmTiEg46AiCiL6z2MxSzSy94zFwAbC6224vAbPNLM7MUoCZwLpg1SQiEi4aWj3ExhjJ8bGhLiWoXUN5wAtm1nGcJ51zr5vZzQDOufudc+vM7HVgFeADHnTOdQ8LEZGI0zG9ROAzMqSCFgTOuRJ66OZxzt3f7fldwF3BqkNEJByFyzxD0MuuoUA3T0zg8Xgz+4KZhf5St4jIMSpcpqCG3l8jWAQkmVkBsBC4FngkWEWJiES6ujCZghp6HwTmnGsCrgD+4Jz7CjA5eGWJiES2hpb2sBg6Cn0IAjM7Ffga8FrgtdBf6hYROUbVt4bHwvXQ+yC4FbgdeME5t8bMioB3g1aViEiEC6eLxb2qwjn3PvA+QOCi8V7n3PeDWZiISKRyzoVVEPR21NCTZpYRuDFsNbDWzH4c3NJERCJTs8eL1+eOua6h45xzdcDlwN+BQvwjh0REpI86Zh5NSzyGWgRAfOC+gcuBl51zHvwzi4qISB+tLq8FYGhGUogr8ettEDwAbANSgUVmNgqoC1ZRIiKR7OHF28jLSOTMCTmhLgXoZRA4537vnCtwzs1zftuBs4Ncm4hIxNlcUc8Hm/ZyzcxRxMeGem0wv95eLM40s990LA5jZr/G3zoQEZE+eOSjbSTExXD1zJGhLmW/3sbRQ/gXof9q4KsOeDhYRYmIRKLaZg9/XVrGF07KZ3BaYqjL2a+3l6zHOOe+1On5nWa2Igj1iIhErGeX7KTZ4+WG00eHupQuetsiaDaz2R1PzOx0oDk4JYmIRB6vz/Hnj7cxo3AQk/MzQ11OF71tEdwMPGpmHdVXA9cFpyQRkcizePNeSqubuX3upFCXcoDeTjGxEjjJzDICz+vM7Fb8K4uJiMhhvLZqF2mJcZw7KTfUpRygT2OXnHN1gTuMAX4YhHpERCKOx+vjjbW7OW9SLklhsEZxd0cziDX0C22KiBwDPtqyj5omDxefmB/qUnp0NEGgKSZERHrQ1u7r8vy1VeWkJ8ZxxrghIaro0A4ZBGZWb2Z1PXzVA+EZbSIiIfTehgpO/tlC/v75LsAfCm+s2cP5x+WFZbcQHOZisXMufaAKERE51rV7ffzitXU0tXn50V9WUpSTRnltM7XNHi4+cVioyzuo8JjoQkQkAjxbXMrmigZ+ftlkUhPjmP9YMU9/toP0pDhmh2m3EPT+PgIRETmExtZ2fvvWRqaNyuaaWaM4Lj+DqxZ8wvZ9TVwxtYDEuPDsFgK1CERE+sUfPyihsr6V2+dNwsw4ZdQgfvqFycQYfGnq8FCXd0hqEYiIHKWK+hYWLCph3glDOWVU9v7XvzZzFBefMIyslIQQVnd4ahGIiByl/35tHR6vjx9fOPGAbeEeAqAgEBE5Kos37+XFFeV8+8wxFA45NpdpURCIiByhFo+XO15czajBKXzn7LGhLueI6RqBiMgReuD9ErbubeTRb8wI25vFekMtAhGRI7BjXxP3vreZS0/KZ8748FiE/kgpCEREjsD7Gytoa/fxLxeMD3UpR01BICJyBMpqWkiIjWFEdkqoSzlqCgIRkSNQVtPMsKwkYmKO/Rn5FQQiIkegvKaZ/MzkUJfRLxQEIiJHoKy6mYJsBcFhmdk2M/vczFaYWfEh9ptuZu1m9uVg1iMi0h88Xh976lvIz4qMIBiI+wjOds7tPdhGM4sF/i+wcABqERE5artrW3AOCrKSQl1KvwiHrqHvAX8FKkJdiIhIb5TVNANQkHXsjxiC4AeBAxaa2VIzm999o5kVAF8E7jvUm5jZfDMrNrPiysrKIJUqItI75YEgyFeLoFdmO+emAnOBW8xsTrftdwO3Oed8B/xkJ865Bc65ac65aTk5x/YdfCJy7Cur7ggCXSM4LOdcWeB7hZm9AMwAFnXaZRrwtJkBDAHmmVm7c+7FYNYlInI0ymubGZKWcEzPL9RZ0ILAzFKBGOdcfeDxBcDPOu/jnCvstP8jwKsKAREJd2U1kTNiCILbIsgDXgj8tR8HPOmce93MbgZwzt0fxGOLiARNWXUT4/PSQ11GvwlaEDjnSoCTeni9xwBwzl0frFpERPqLc47ymhbOmpAb6lL6TTgMHxUROWZUN3lo9ngpiKCuIQWBiEgf/GPoqIJARCQq/eNmMgWBiEhU6riHIFImnAMFgYhIn5TXNJMUH0N2SnyoS+k3CgIRkT4oq2kmPyuZwND4iKAgEBHpg/Ka5oi6PgAKAhGRPilTEIiIRK8Wj5e9DW0KgkjT2u7l4cVbaWs/5ASoIiLsqm0BIuseAlAQ8ObaPdz5yloWbdQ6ByJyaJE2/XSHqA+CJVurANhR1RTiSkQk3G3b1wjA8Ai6hwAUBHy2rRpQEIjI4b2xZjfDs5N1jSCS1DZ7WL+7DoDSagWBiBzc7toWPty8lyumFBATEzn3EECUB8Gy7dU4B4NSE9QiEJFDemlFGc7BF6cOD3Up/S6qg+CzbVXExRhzjx/KjqomnHOhLklEwtQLy8uYMjKLwiGpoS6l30V1ECzZWsXxBZlMGJpOi8dHZUNrqEsSkTC0tryO9bvruWJKQahLCYqoDYIWj5dVpbXMKBzEiEEpAOxU95CI9OD5ZaXExxqXnJgf6lKCImqDYFVpLW1eH9NHD2JEtj8IdJ1ARLpr9/p4aWU5Z0/IJTs1IdTlBEXUBsGSbf77B6aNyt4/JnhnVfNB96+sb9U1BJEotHjLPirrW7liamR2C0EUB8FnW6sYn5dGdmoCSfGxDM1IOmiLYMe+Jk775ds8/sn2Aa5SRELtkcVbGZyawNkTI2ex+u6iMgi8Psey7dVMHz1o/2sjB6UcNAheWVWOx+u4//0SPF7NSSQSLTbuqefdDZVcd9poEuNiQ11O0ERlEKzbVUd9a3uXIBgxKOWgF4tfWVlORlIcZTXNvLqqfKDKFJEQW7CohOT4WK6dNSrUpQRVVAZBx/WB6YVdWwS761po8Xi77Lu5ooH1u+v5/rnjGJ+XxgPvl+y/VlDT1Mb3n1rOpyX7Bq54ERkQu2tbeGlFGVdOHxGxF4k7xIW6gFBYsq2Kgqyu84WMGJSMc/5FJ8bkpO1//bVVuzCDS07MJzslgR/9ZSXvbaxk2qhsrnvoM1aW1rJhdz1//+czIu62czk0r8+xsrQGn8+RGBdLUnwM6UnxZCTHkRwfG1FLGUajhxdvxefgxtmFoS4l6KIuCJxzLNlWzeljBnd5fWSnewk6B8Grq8qZPmoQQzOTuPSkfH61cAP3vrOZGDPWlNdx5bQRPFO8kzfW7GbuCcMG9N8ifdPQ2s7OqiZKq5vZXdtMXUs7dc0e9jW2UVbdTGlNE9WNHnLTE8nPSmbU4BSumFrA1JHZXT7U9zW08kzxTp74ZAdlNT2PNEuIjSEnPZH8rCSGZSYze+wQLjphKBlJkbPgeSSra/HwxKc7mHfCsP33GUWyqAuC7fuaqKxv7dItBF2DoMOG3fVsqmjgZ5dNBiAhLoYbZxfyi9fWEWPwu6umMO+EYSzZXsXv3t7EhZOHHnGrwOdzmKG/IvuZz+d4Z30Ff/yghE8DU453lhQfQ1ZyAgXZyUwdmU12SgKV9a2U1TTz4vIynvh0B8cXZHD5yQWUVjezfEc1a8rraPc5Ti0azP+5aALZKQm0tvto8Xipb2mnttlDTXMbFXWtlNc089nWKl5eWc4dL63mnAm5pCTEUlbTTEV9K6ePHcwPz5/AoAjvejjWPFdcSkNrO9+aUxTqUgZE1AXBZ4HrAzNGdw2CnPREEuNiuowcem1VOTEGFx0/dP9rV80YyfsbK7liagGXnuS/y/B754zlB8+s5M11e7hw8lB6a29DKwvX7GHRxkoWb9nLkLREbrtoIhdOzlMgHKHqxjZWltawo6qJ7fuaeHdDBSWVjeRnJnHreeMYm5vGiOwUhmUlkZkcf8iRII2t7Ty/vIxHP9rGL15bR3J8LCcOz2T+nCK+OKWAcXnpvarJOceKnTW8uLyM19fsJi4mhvysJIqGpPLUZzt5ZeUufnj+eK6eOZL42Ki8bBd23lizm4lD0zm+IDPUpQyIqAuCJVuryE6JZ2xuWpfXzazLEFLnHK+u2sXMwsHkpift3y8tMY7HbpzZ5WcvPTGf3721id+/vYkLjuvdh/iu2mYuu2cxFfWt5GcmMff4oSzfUcPNjy9lZuEg5ozPYeXOGpbvrCE/M4k/XHNK2MyB7vU5lu2o5s21e3h/QyVVTW00t3nxeH1cfnIBt8+bSFbKwPyF65xjU0UDb6+r4O11e1i2oxpf4L6/pPgYjhuWwe+uOpl5Jwzr84dsamIc184axTUzR1Ja3cywzCTijuCD2syYMjKbKSOzufOy47ts27innjtfWcN/vryGu9/ayPnH5TH3+GHMKhpMckLkDlcMZ7VNHoq3V3PzmdHRGoBoDIJtVUwbPajHD2t/EPj7fF9aUU7J3kZuOuPwvwxxsTF895xx/MtfVrJw7eFbBU1t7dz052Ka2rz89dun7u+Dbvf6eGrJTn775kY+3VpF4ZBUTh8zmLfXV3D5vYt56LrpnDA8k217G3lg0RZWl9UxLDOJ/Kxk8rOSGJSayOC0BAoHpzK62wyJFfUt/G3VLrbta2JnVRO7altobffS2u4jKT6W2y6ayPnH5e3f/821e7j33c3ExRhZKQmkJ8VR3dTGnrpWyqqbqGtpJz7WmFU0mKmjskiOj6Oh1cNzy0p5e/0e/uOS4/jCSfn90rLZsa+J9zZW8P6GSmqbPQzPTmZ4dgrVTW28t6Fyfz/95PwMvnvOOE4fM5jCIankpCf2y/HNLGj9xOPz0nn8xpm8t7GSF5eX8ffPd/NscSkxBmNy0picn8EFk4dy0VF0O0rfLNpUidfnOGdi3uF3jhB2rE2bMG3aNFdcXHxEP1tR38KM/3qbf583iW/20Pf305fX8NzSUh67cQZXLviEk0dk8fiNM0mIO/xfge1eHxfevYjWdh8LfzCHlISeM9bnc3zniWUsXLubP103vce7FVs8Xlo83v1/VW/cU88NDy+hqrGNM8YN4a11e4iLjWH66Gx/f3Z1M41tXYe9Xjg5j1vPG0/hkFQe/KCEP7y3haY2L6kJsYwcnEp+ZhJJCbEkxsXsn1nx6pkjufW8cfzqjQ08W1xKUU4qQzOSqG7yUN/iITslgdz0RPIykzi1aDBnTcghvdvFz7Xlddz+/CpWltaSl5HIrKLBnFo0mCkjsxmTk3rAX9T1LR4+2rKPDzftpaqxjbhYIy4mhvoWD7vrWiivaWFvYFbYUYNTGJaZRGl1M7tqW0iKi+H0sUM4e2IuZ03IYVhmeLSYjkZru5ePtuxj+Y4a1pTVsqqslsr6VibnZ/DjCydw5vgcdRsG2Q+eWcH7GytZ8u/nERtB4WtmS51z03rcFk1B8NqqXdzy5DJe+M5pTBmZfcD2P324lZ+/upZBqQmkJsby0i2z+3QR75OSfVy14BO+NaeI2+dN6nGfu95Yz73vbuGOiyf1qrXRoaK+hZv+XMzmigaumTWKm2YXkpvh77JyztHQ2k5VYxt7G9p4f2MlD3+4lfrWdgalJlDV2MZFk4fy44smUDQk9YAPktZ2L79ZuJEFH5TQseXmM8dw63njexWC3Xl9jheXl/Huhgo+Kana/0GeGBfDxGEZpCXG0tzmpanNy+aKBtp9jtSEWIZmJtHuc7R7HSkJsQzLSiY/M4kJQ9M5a0Jul3ng2wN3eB9JV82xxOtzvLSijN++tZGdVc3MKBzEbRdN4JRRgw7/w9JnXp9j2i/e5OwJufzmypNDXU6/OlQQRFXX0JJtVSTHxx70AlDHyKFWj5en58/q80iOWUWDuWr6CB78cCuXnpR/wHGe+mwH9767haumj+jz2OTc9CSe//ZptHl9B7Q2zIz0pHjSk+IZNTiVU0Zlc+PphTz4YQnrdtVz0xmFzCoafJB3hsS4WG6fN4kzx+fwyEfb+NaZRUf1QRMbY3zplOF86ZThOOco2dvI56W1rC6rZU15Ha0eH6mJcQxKTeSsCbmcOT6HU0Zl9yl0Ij0AOsTGGFdMHc4lJ+bzzJId/P6dzXzpvo85d2IuPzh/fNRczBwoK3ZWU93kieh5hXoSVS2Ci3//AZnJ8Tz5zVk9bt9T18JXH/iY/7z0uCPuH6xt8nDub95nWGYSL3zntP0fWO+s38M3H13K7LFDePC6aRodIkekqa2dhxdv4/73t1Df0s6M0YO4/vTRXHBcXtSEYzDd9cZ67n+/hGX/cT6ZyZF1z4e6hvD3RZ9050K+d844fnD++CBU9g+vrCzne08tZ1bRIM4Yl0N+VhL/9vxqxuam8fT8WaQmRlVDTIKgttnDX4p38shH2yitbmZoRhJXTh/BVTNGRMS1klC56O5FZKXE8/T8U0NdSr9T1xCwdLt/WOGMwuD3rV5y4jC2VDbw6qpd3PXGBsA/hcVD109XCEi/yEyO56Yzirjh9ELeXreHxz/dwe/f2cT/vrOJeScM4yeXHLf/GpL0TllNM+t31/Nv8yaGupQBF9RPJTPbBtQDXqC9exqZ2deA2wAL7Pdt59zKYNQyJC2Rf5oxgikjs4Lx9l2YGbeeN55bzxtPdWMbn5fVcnxBpu4elX4XG2NcMHkoF0weys6qJp74dAcPLd7K+xsruePiSXx12giNMuqld9ZXAETVsNEOQe0aCgTBNOfc3oNsPw1Y55yrNrO5wE+dczN72rfD0VwjEIkGJZUN/Ovzn/PZ1irOmpDDvVdPVUu0Fy6750Oa2rws/MGciAzPQ3UNhfTqknPuI+dcdeDpJ8DwUNYjEgmKctJ4+puzuPMLk1m0sZKvP/QZdS2eUJcV1lbsrGFlaS3XnjoqIkPgcIIdBA5YaGZLzWz+Yfa9Efh7TxvMbL6ZFZtZcWVlZb8XKRJpYmKM604bzb1XT2VVaQ1X//ETqhrbQl1W2Hr0o22kJcZxxdTo/Fs02EEw2zk3FZgL3GJmc3rayczOxh8Et/W03Tm3wDk3zTk3LScnJ3jVikSYuScMY8G109i0p4Gr//gJTW3toS4p7OxraOXVVbu4YmoBaVHahRbUIHDOlQW+VwAvADO672NmJwIPApc557TUl0g/O3tiLg9cewob9tRzxwurOdaGjAfb00t20ub18fVTI3s5ykMJWhCYWaqZpXc8Bi4AVnfbZyTwPHCtc25jsGoRiXZnTcjln88dx/PLy3i2eGeoywkb7V4fT366g9PGDGZsbu+mFY9EwWwR5AEfmtlK4DPgNefc62Z2s5ndHNjnJ8Bg4A9mtsLMNBxIJEi+d844Zo8dwk9eWsPa8rpQlxMWXvt8F2U1zXz91NGhLiWkoubOYhHxL4Y073cfkBQfyxM3zYyKZRh7UlHfwl2vb+C5ZaUUDkll4a1zIn6KjrAdPioiA2tIWiIPXHsKNU1tfPEPH7G6rDbUJQ2455eVcvZd7/HiijLmn1HES7ecHvEhcDjR/a8XiUJTRmbz12+fRkKsceUDH7NoY/QMyX5r7R7+5S8rOb4gk4U/OJPb5006YE2NaKQgEIlC4/LSef47pzNiUAo3/bmYpdurQl1S0K3YWcN3n1rG8QWZPHT99C7rW0Q7BYFIlBqamcTT82cxLCuJbz22jPLAkp+RpsXjZcm2Kr7xyBJy05P403Wa/LE7BYFIFMtKSeDBr0+jxeNl/mPFNHdb8vRY9tCHWznrrnc57iev85X7P8Y5xyM3TCcnPTHUpYUdBYFIlBuXl87vrjqZNeV1/Pi5lfh8x9ZIwp58uGkvP3t1LUPSEvneOeO45+opvPGDORTlpIW6tLCk9pGIcO6kPG67aCK//Pt6ctIT+cklx4Xt5GstHi+3Pr2Cguxk/n3eJGK6LTBf1djGD59dwdjcNB67cSbJCbEhqvTYoSAQEQC+NaeIirpWHlq8lczkeG49L3gr+e2sauKlFWXMLBrM9NG9XyzK63P88NkVvL5mNwAer487vzB5f2g557jtr6uoafLw8A3TFQK9pCAQEcC/oNIdF0+irsXD3W9tIiMpnm/MLuzXY2yuqOcP727hpZXleANdUJefnM/t8yaRd5gV1Zxz/PzVtfzt893ccfEk9tS18McPtpIUH8vtcyeyuaKBvy4r4821e7jj4klMzs/s19ojmYJARPaLiTF+ecUJ1Ld4eOSxt5l730cMe+U5aGiAtDS45hr40Y9gzJg+v/ea8louv3cxcTExXH/aaK6ZNYoXlpVy/6IS3ly7h5vPHMMNswu7zABaXtPMtr2NVNS3snR7NY99sp0bZxdy0xlFOOdobfexYFEJzyzZSW2zf82FCyfn8Y3T+zfAIp2mmBCRA7S+/Cq+L3+FOK+HeF+nkUTx8f6v556DuXN7/X5en+OK+z6irLqJv/3zGeSm/+Ov/+37Gvmv19axcO0eBqUmcPOZRXi8jr99vos13eZEumJqAb/68kn7rwv4fI573t3M1r2NzCoaxKlFQxgxKDlsr2+E0qGmmFAQiEhXW7bAiSdCU9PB90lJgVWret0yeOyT7fzHi6u5+8qTuXxKQY/7LN9Rza8XbuTDzf6VbaeMzOKiyUM5cXgWuRmJ5KQnkqG7gI/YoYJAXUMi0tWvfw2ewyxt6fHAb38L99wD+PvvV5fVMWlY+gHz9lTUt/D/Xl/P6WMHc9nJ+Qd9yykjs3n8ppms21VHVko8wzKTj/qfIr2j+whEpKvHH+9dEDz22P6nv31rE5fe8yEX3r2IN9fu2b/4TXOblztfWUurx8fPLzu+V102k4ZlKAQGmFoEItJVQ0Of9nt+WSm/f3sT507MZeu+Rr75aDEnDs+krd3Hxj31+Bzcet443cwVxhQEItJVWhrU1/dqv09L9nHbX1dxatFg7rvmFMz8Sz8+8cl28jKSOP+4PE4ekcXZE3KDX7ccMQWBiHR1zTXw4IOH7B5qj41j0fQLuPXRYkYMSuH+a04hIc7f03ztrFFcOyt61/89FukagYh09aMf+YeIHoInJo4nTvsSJ43I4uHrp5OZotE8xzK1CESkqzFj/PcJfPnL/lZB55ZB4D6C5Oee4099uI9AwptaBCJyoLlz/fcJzJ8PGRkQE+P/Pn++/3WFQETRDWUiIlFAi9eLiMhBKQhERKKcgkBEJMopCEREopyCQEQkyikIRESi3DE3fNTMKoHtoa6jnw0B9oa6iGOMzlnf6Hz1TSSer1HOuZyeNhxzQRCJzKz4YON7pWc6Z32j89U30Xa+1DUkIhLlFAQiIlFOQRAeFoS6gGOQzlnf6Hz1TVSdL10jEBGJcmoRiIhEOQWBiEiUUxCIiEQ5BUGYM7MzzOx+M3vQzD4KdT3hzszOMrMPAufsrFDXE+7MbFLgXD1nZt8OdT3hzsyKzOxPZvZcqGvpTwqCIDKzh8yswsxWd3v9IjPbYGabzexfD/UezrkPnHM3A68Cfw5mvaHWH+cLcEADkASUBqvWcNBPv1/rAr9fXwVOD2a9odZP56vEOXdjcCsdeBo1FERmNgf/h9KjzrnjA6/FAhuB8/F/UC0B/gmIBf6n21t8wzlXEfi5Z4EbnXP1A1T+gOuP8wXsdc75zCwP+I1z7msDVf9A66/fLzP7AvBt4DHn3JMDVf9A6+f/H59zzn15oGoPNi1eH0TOuUVmNrrbyzOAzc65EgAzexq4zDn3P8AlPb2PmY0EaiM5BKD/zldANZAYlELDRH+dL+fcy8DLZvYaELFB0M+/XxFFXUMDrwDY2el5aeC1Q7kReDhoFYW3Pp0vM7vCzB4AHgPuCXJt4aiv5+ssM/t94Jz9LdjFhaG+nq/BZnY/MMXMbg92cQNFLYJjgHPuP0Ndw7HCOfc88Hyo6zhWOOfeA94LcRnHDOfcPuDmUNfR39QiGHhlwIhOz4cHXpOe6Xz1jc5X3+h8oSAIhSXAODMrNLME4Crg5RDXFM50vvpG56tvdL5QEASVmT0FfAxMMLNSM7vROdcOfBd4A1gHPOucWxPKOsOFzlff6Hz1jc7XwWn4qIhIlFOLQEQkyikIRESinIJARCTKKQhERKKcgkBEJMopCEREopyCQCKGmTUM8PEGdH0IM8sys+8M5DElOigIRA7CzA45F5dz7rQBPmYWoCCQfqcgkIhmZmPM7HUzWxpYuWxi4PVLzexTM1tuZm8F1i/AzH5qZo+Z2WLgscDzh8zsPTMrMbPvd3rvhsD3swLbnzOz9Wb2hJlZYNu8wGtLA7N8vtpDjdeb2ctm9g7wtpmlmdnbZrbMzD43s8sCu/4SGGNmK8zsrsDP/tjMlpjZKjO7M5jnUiKYc05f+oqIL6Chh9feBsYFHs8E3gk8zuYfd9bfBPw68PinwFIgudPzj/CvbTAE2AfEdz4ecBZQi3/Cshj80xjMxr9K2k6gMLDfU8CrPdR4Pf7pjwcFnscBGYHHQ4DNgAGjgdWdfu4CYEFgWwz+VezmhPq/g76OvS9NQy0Ry8zSgNOAvwT+QId/LFYzHHjGzIYBCcDWTj/6snOuudPz15xzrUCrmVUAeRy4DOZnzrnSwHFX4P/QbgBKnHMd7/0UMP8g5b7pnKvqKB3478CKWj788+Pn9fAzFwS+lgeepwHjgEUHOYZIjxQEEsligBrn3Mk9bPtf/EtZvmz+Re5/2mlbY7d9Wzs99tLz/ze92edQOh/za0AOcIpzzmNm2/C3Lroz4H+ccw/08VgiXegagUQs51wdsNXMvgJgficFNmfyj3nnrwtSCRuAok7LI17Zy5/LBCoCIXA2MCrwej2Q3mm/N4BvBFo+mFmBmeUefdkSbdQikEiSYmadu2x+g/+v6/vM7A4gHngaWIm/BfAXM6sG3gEK+7sY51xzYLjn62bWiH/u+954AnjFzD4HioH1gffbZ2aLzWw18Hfn3I/NbBLwcaDrqwG4Bqjo73+LRDZNQy0SRGaW5pxrCIwiuhfY5Jz7bajrEulMXUMiwfXNwMXjNfi7fNSfL2FHLQIRkSinFoGISJRTEIiIRDkFgYhIlFMQiIhEOQWBiEiUUxCIiES5/w+RySiHfCQTMwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Init ModelCheckpoint callback, monitoring 'val_loss'\n",
    "callbacks = [\n",
    "    ModelCheckpoint(monitor=\"val_acc_epoch\"),\n",
    "    EarlyStopping(monitor=\"valid_loss\", patience=5, verbose=False)\n",
    "]\n",
    "  \n",
    "trainer = pl.trainer.Trainer(\n",
    "    max_epochs=hparams[\"epochs\"],\n",
    "    gpus=1,\n",
    "    callbacks=callbacks,\n",
    "    accelerator=\"gpu\",\n",
    "    auto_lr_find=True\n",
    ")\n",
    "# Run learning rate finder\n",
    "lr_finder = trainer.tuner.lr_find(model, train_dataloaders=data_loader)\n",
    "\n",
    "# Results can be found in\n",
    "lr_finder.results\n",
    "\n",
    "# Plot with\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()\n",
    "\n",
    "# Pick point based on plot, or get suggestion\n",
    "new_lr = lr_finder.suggestion()\n",
    "print(f\"Suggested LR: {new_lr}\")\n",
    "\n",
    "# update hparams of the model\n",
    "model.learning_rate = new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name     | Type     | Params\n",
      "--------------------------------------\n",
      "0 | accuracy | Accuracy | 0     \n",
      "1 | backbone | ResNet   | 21.4 M\n",
      "--------------------------------------\n",
      "21.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.4 M    Total params\n",
      "85.451    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  31%|███       | 144/465 [01:19<02:56,  1.81it/s, loss=2.12, v_num=21]"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_loader, val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, test_dataloaders=val_data_loader)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4ea5d56035456c2b1b6b90ed23f43434caf1e4d5f44691eca1785db1c6f7643"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
