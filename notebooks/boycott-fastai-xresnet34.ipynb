{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import noisereduce as nr\n",
    "import json\n",
    "import numpy as np\n",
    "# import modin.pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "torchaudio.set_audio_backend(\"sox_io\")\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATIONS\n",
    "IS_KAGGLE = False\n",
    "if IS_KAGGLE:\n",
    "    ROOT_DIR = Path('../../input/birdclef-2022')\n",
    "else:\n",
    "    ROOT_DIR = Path('../../datasets/birdclef-2022')\n",
    "DATA_DIR = ROOT_DIR / \"train_audio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "\n",
    "def crop_or_pad(y, length, sr, train=True, offset=0, probs=None):\n",
    "    \"\"\"\n",
    "    Crops an array to a chosen length\n",
    "    Arguments:\n",
    "        y {1D np array} -- Array to crop\n",
    "        length {int} -- Length of the crop\n",
    "        sr {int} -- Sampling rate\n",
    "    Keyword Arguments:\n",
    "        train {bool} -- Whether we are at train time. If so, crop randomly, else return the beginning of y (default: {True})\n",
    "        probs {None or numpy array} -- Probabilities to use to chose where to crop (default: {None})\n",
    "    Returns:\n",
    "        1D np array -- Cropped array\n",
    "    \"\"\"\n",
    "    if len(y) <= length:\n",
    "        y = np.concatenate([y, np.zeros(length - len(y))])\n",
    "    else:\n",
    "        if not train:\n",
    "            start = 0 + offset\n",
    "        elif probs is None:\n",
    "            start = np.random.randint(len(y) - length)\n",
    "        else:\n",
    "            start = (\n",
    "                    np.random.choice(np.arange(len(probs)), p=probs) + np.random.random()\n",
    "            )\n",
    "            start = int(sr * (start))\n",
    "\n",
    "        y = y[start: start + length]\n",
    "\n",
    "    return torch.from_numpy(y).float()\n",
    "\n",
    "\n",
    "def mono_to_color(X, eps=1e-6, mean=None, std=None):\n",
    "    \"\"\"\n",
    "    Converts a one channel array to a 3 channel one in [0, 255]\n",
    "    Arguments:\n",
    "        X {numpy array [H x W]} -- 2D array to convert\n",
    "    Keyword Arguments:\n",
    "        eps {float} -- To avoid dividing by 0 (default: {1e-6})\n",
    "        mean {None or np array} -- Mean for normalization (default: {None})\n",
    "        std {None or np array} -- Std for normalization (default: {None})\n",
    "    Returns:\n",
    "        numpy array [3 x H x W] -- RGB numpy array\n",
    "    \"\"\"\n",
    "    X = np.stack([X, X, X], axis=-1)\n",
    "\n",
    "    # Standardize\n",
    "    mean = mean or X.mean()\n",
    "    std = std or X.std()\n",
    "    X = (X - mean) / (std + eps)\n",
    "\n",
    "    # Normalize to [0, 255]\n",
    "    _min, _max = X.min(), X.max()\n",
    "\n",
    "    if (_max - _min) > eps:\n",
    "        V = np.clip(X, _min, _max)\n",
    "        V = 255 * (V - _min) / (_max - _min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        V = np.zeros_like(X, dtype=np.uint8)\n",
    "\n",
    "    return V\n",
    "\n",
    "def compute_deltas(sg, width=9, order=1):\n",
    "    def librosa_delta(data, order=1, width=9):\n",
    "        return torch.from_numpy(librosa.feature.delta(data.numpy(), order=order, width=width))\n",
    "\n",
    "    if sg.shape[1] < width:\n",
    "        raise ValueError(\n",
    "            f\"\"\"Delta not possible with current settings, inputs must be wider than\n",
    "        {width} columns, try setting max_to_pad to a larger value to ensure a minimum width\"\"\"\n",
    "        )\n",
    "    new_channels = [\n",
    "        torch.stack([c, librosa_delta(c, order=1), librosa_delta(c, order=2)]) for c in sg\n",
    "    ]\n",
    "    sg.data = torch.cat(new_channels, dim=0)\n",
    "    return sg\n",
    "\n",
    "audio2mfcc = torchaudio.transforms.MFCC(n_mfcc=64, melkwargs={'n_fft':2048, 'hop_length':256, 'n_mels':128})    \n",
    "\n",
    "@torch.no_grad()\n",
    "def create_mfcc(\n",
    "        fname: str,\n",
    "        reduce_noise: bool = False,\n",
    "        frame_size: int = 5,\n",
    "        sr: int = 16000,\n",
    "        train: bool = True,\n",
    "    ) -> list:\n",
    "    waveform, sample_rate = torchaudio.load(fname)\n",
    "    # print(f\"Input: {waveform.size()}\")\n",
    "\n",
    "    # DownMixMono: convert to one channel. Some files have two channels\n",
    "    waveform = waveform.contiguous().mean(-2).unsqueeze(-2).float()\n",
    "    # print(f\"DownMixMono: {waveform.size()}\")\n",
    "\n",
    "    # Convert to 16000Hz\n",
    "    resample = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=sr)\n",
    "    waveform = resample(waveform)\n",
    "    # print(f\"Resample: {waveform.size()}\")\n",
    "\n",
    "    waveform = crop_or_pad(\n",
    "        waveform.squeeze().numpy(),\n",
    "        length=frame_size * sr,\n",
    "        sr=sr,\n",
    "        train=train\n",
    "    )\n",
    "    waveform = waveform.unsqueeze(0)\n",
    "    # print(f\"ResizeSignal: {waveform.size()}\")\n",
    "    if reduce_noise:\n",
    "        waveform = torch.tensor(nr.reduce_noise(\n",
    "            y=waveform, sr=sample_rate, win_length=256, use_tqdm=False, n_jobs=2,\n",
    "        ))\n",
    "\n",
    "    # print(f\"Before mfcc: {waveform.size()}\")\n",
    "    spectrograms = audio2mfcc(waveform)\n",
    "    # print(f\"MFCC: {spectrograms.size()}\")\n",
    "    mfcc = compute_deltas(spectrograms)\n",
    "    # print(f\"Delta: {spectrograms.size()}\")\n",
    "    mfcc = mfcc.float().div_(255.)\n",
    "    return mfcc\n",
    "\n",
    "@torch.no_grad()\n",
    "def create_mfcc_inference(\n",
    "        fname: str,\n",
    "        reduce_noise: bool = False,\n",
    "        frame_size: int = 5,\n",
    "        sr: int = 16000\n",
    "    ):\n",
    "    waveform, sample_rate = torchaudio.load(fname)\n",
    "    # print(f\"Input: {waveform.size()}\")\n",
    "\n",
    "    # DownMixMono: convert to one channel. Some files have two channels\n",
    "    waveform = waveform.contiguous().mean(-2).unsqueeze(-2).float()\n",
    "    # print(f\"DownMixMono: {waveform.size()}\")\n",
    "\n",
    "    # Convert to 16000Hz\n",
    "    resample = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=sr)\n",
    "    waveform = resample(waveform)\n",
    "    # print(f\"Resample: {waveform.size()}\")\n",
    "    \n",
    "    nb = int(frame_size * sr)\n",
    "    clip_mfccs = []\n",
    "    for i in range(ceil(waveform.size()[-1] / nb)):\n",
    "        cropped_waveform = crop_or_pad(\n",
    "            waveform.squeeze().numpy(),\n",
    "            length=frame_size * sr,\n",
    "            sr=sr,\n",
    "            train=False,\n",
    "            offset=i*sr\n",
    "        )\n",
    "        cropped_waveform = cropped_waveform.unsqueeze(0)\n",
    "        # print(f\"ResizeSignal: {cropped_waveform.size()}\")\n",
    "        if reduce_noise:\n",
    "            cropped_waveform = torch.tensor(nr.reduce_noise(\n",
    "                y=cropped_waveform, sr=sample_rate, win_length=256, use_tqdm=False, n_jobs=2,\n",
    "            ))\n",
    "        # print(f\"Before mfcc: {cropped_waveform.size()}\")\n",
    "        spectrograms = audio2mfcc(cropped_waveform)\n",
    "        # print(f\"MFCC: {spectrograms.size()}\")\n",
    "        mfcc = compute_deltas(spectrograms)\n",
    "        # print(f\"Delta: {spectrograms.size()}\")\n",
    "        mfcc = mfcc.float().div_(255.)\n",
    "        clip_mfccs.append(mfcc)\n",
    "    return torch.stack((clip_mfccs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "torch.Size([3, 64, 313])\n"
     ]
    }
   ],
   "source": [
    "# Debug\n",
    "fname = ROOT_DIR /\"test_soundscapes/soundscape_453028782.ogg\")\n",
    "# fname = ROOT_DIR / \"train_audio/dunlin/XC588428.ogg\")\n",
    "spec = create_mfcc_inference(fname)\n",
    "print(len(spec))\n",
    "print(spec[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 available classes\n",
      "Dataset loaded with 14852 files\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class BirdCLEFDataset(Dataset):\n",
    "    def __init__(self, root_dir='/kaggle/input/birdclef-2022', is_training=False, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.is_training = is_training\n",
    "        if not is_training:\n",
    "            self.data_dir = os.path.join(self.root_dir, 'test_soundscapes')\n",
    "            self.label_file = os.path.join(self.root_dir, 'test.csv')\n",
    "            \n",
    "            # Preparing a sample submission file just to save the notebook and submit for the competition.\n",
    "            # Note : When the notebook will be submitted the below cells will be working and the genuine output file will be produced.\n",
    "            df = pd.read_csv(self.label_file)\n",
    "            test = df.copy()\n",
    "            test[\"target\"] = [False for _ in range(len(test))]\n",
    "            imp_features = [\"row_id\", \"target\"]\n",
    "            test = test[imp_features]\n",
    "            test.to_csv(\"submission.csv\", index = False)\n",
    "        else:\n",
    "            self.data_dir = os.path.join(self.root_dir, 'train_audio')\n",
    "            self.label_file = os.path.join(self.root_dir, 'train_metadata.csv')\n",
    "\n",
    "        # Retrieve categories and its label name\n",
    "        self.class_names = [\n",
    "            \"afrsil1\",\"apapan\",\"bkwpet\",\"brnowl\",\"cacgoo1\",\"chemun\",\"comwax\",\n",
    "            \"fragul\",\"grefri\",\"hawgoo\",\"iiwi\",\"layalb\",\"lotjae\",\"merlin\",\"norsho\",\n",
    "            \"parjae\",\"reccar\",\"ribgul\",\"saffin\",\"sooshe\",\"wantat1\",\"whttro\",\"akekee\",\n",
    "            \"arcter\",\"blkfra\",\"brtcur\",\"calqua\",\"chukar\",\"coopet\",\"gadwal\",\"gresca\",\n",
    "            \"hawhaw\",\"incter1\",\"lcspet\",\"madpet\",\"mitpar\",\"nutman\",\"pecsan\",\"redava\",\n",
    "            \"rinduc\",\"sander\",\"sooter1\",\"warwhe1\",\"wiltur\",\"akepa1\",\"barpet\",\"blknod\",\n",
    "            \"bubsan\",\"cangoo\",\"cintea\",\"crehon\",\"gamqua\",\"gryfra\",\"hawpet1\",\"jabwar\",\n",
    "            \"leasan\",\"magpet1\",\"moudov\",\"oahama\",\"peflov\",\"redjun\",\"rinphe\",\"semplo\",\n",
    "            \"sopsku1\",\"wesmea\",\"yebcar\",\"akiapo\",\"bcnher\",\"bongul\",\"buffle\",\"canvas\",\n",
    "            \"comgal1\",\"dunlin\",\"glwgul\",\"gwfgoo\",\"hoomer\",\"japqua\",\"leater1\",\"mallar3\",\n",
    "            \"norcar\",\"omao\",\"perfal\",\"redpha1\",\"rocpig\",\"sheowl\",\"sora\",\"wessan\",\"yefcan\",\n",
    "            \"akikik\",\"belkin1\",\"brant\",\"bulpet\",\"caster1\",\"commyn\",\"elepai\",\"gnwtea\",\n",
    "            \"hawama\",\"houfin\",\"kalphe\",\"lessca\",\"masboo\",\"norhar2\",\"osprey\",\"pibgre\",\n",
    "            \"refboo\",\"rorpar\",\"shtsan\",\"spodov\",\"wetshe\",\"zebdov\",\"amewig\",\"bkbplo\",\n",
    "            \"brnboo\",\"burpar\",\"categr\",\"compea\",\"ercfra\",\"golphe\",\"hawcoo\",\"houspa\",\n",
    "            \"kauama\",\"lesyel\",\"mauala\",\"normoc\",\"pagplo\",\"pomjae\",\"rempar\",\"rudtur\",\n",
    "            \"skylar\",\"sposan\",\"whfibi\",\"aniani\",\"bknsti\",\"brnnod\",\"buwtea\",\"chbsan\",\n",
    "            \"comsan\",\"eurwig\",\"grbher3\",\"hawcre\",\"hudgod\",\"laugul\",\"lobdow\",\"maupar\",\n",
    "            \"norpin\",\"palila\",\"puaioh\",\"rettro\",\"ruff\",\"snogoo\",\"towsol\",\"whiter\"\n",
    "        ]\n",
    "        self.num_classes = len(self.class_names)\n",
    "        print('{:d} available classes'.format(self.num_classes))\n",
    "\n",
    "        # Load image ids\n",
    "        self.sound_files = []\n",
    "        self.labels = []\n",
    "        label_df = pd.read_csv(self.label_file)\n",
    "        for index, row in label_df.iterrows():\n",
    "            label_name = row[\"primary_label\"]\n",
    "            file_name = row[\"filename\"]\n",
    "            cat_id = self.class_names.index(label_name)\n",
    "            self.sound_files.append(file_name)\n",
    "            self.labels.append(cat_id)\n",
    "\n",
    "        print('Dataset loaded with {:d} files'.format(len(self.sound_files)))\n",
    "\n",
    "        # PyTorch Transform\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sound_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Generate one sample of data\n",
    "        \"\"\"\n",
    "        filename = self.sound_files[index]\n",
    "        file_path = os.path.join(self.data_dir, filename)\n",
    "\n",
    "        # To Tensor\n",
    "        mfcc = create_mfcc(file_path)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = self.labels[index]\n",
    "\n",
    "        return mfcc, label\n",
    "\n",
    "dataset = BirdCLEFDataset(root_dir=ROOT_DIR, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "class BirdCLEFDataLoader(torch.utils.data.DataLoader):\n",
    "    def __init__(self, dataset, batch_size, shuffle, validation_split=0.2, num_workers=4, collate_fn=default_collate):\n",
    "        self.validation_split = validation_split\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.batch_idx = 0\n",
    "        self.n_samples = len(dataset)\n",
    "\n",
    "        self.sampler, self.valid_sampler = self._split_sampler(self.validation_split)\n",
    "\n",
    "        self.init_kwargs = {\n",
    "            'dataset': dataset,\n",
    "            'batch_size': batch_size,\n",
    "            'shuffle': self.shuffle,\n",
    "            'collate_fn': collate_fn,\n",
    "            'num_workers': num_workers\n",
    "        }\n",
    "        super().__init__(sampler=self.sampler, **self.init_kwargs)\n",
    "\n",
    "    def _split_sampler(self, split):\n",
    "        if split == 0.0:\n",
    "            return None, None\n",
    "\n",
    "        idx_full = np.arange(self.n_samples)\n",
    "\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(idx_full)\n",
    "\n",
    "        if isinstance(split, int):\n",
    "            assert split > 0\n",
    "            assert split < self.n_samples, \"validation set size is configured to be larger than entire dataset.\"\n",
    "            len_valid = split\n",
    "        else:\n",
    "            len_valid = int(self.n_samples * split)\n",
    "\n",
    "        valid_idx = idx_full[0:len_valid]\n",
    "        train_idx = np.delete(idx_full, np.arange(0, len_valid))\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "        # turn off shuffle option which is mutually exclusive with sampler\n",
    "        self.shuffle = False\n",
    "        self.n_samples = len(train_idx)\n",
    "\n",
    "        return train_sampler, valid_sampler\n",
    "\n",
    "    def split_validation(self):\n",
    "        if self.valid_sampler is None:\n",
    "            return None\n",
    "        else:\n",
    "            return torch.utils.data.DataLoader(sampler=self.valid_sampler, **self.init_kwargs)\n",
    "\n",
    "data_loader = BirdCLEFDataLoader(dataset, batch_size=32, shuffle=True, num_workers=8)\n",
    "val_data_loader = data_loader.split_validation()\n",
    "print(len(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    \"afrsil1\",\"apapan\",\"bkwpet\",\"brnowl\",\"cacgoo1\",\"chemun\",\"comwax\",\n",
    "    \"fragul\",\"grefri\",\"hawgoo\",\"iiwi\",\"layalb\",\"lotjae\",\"merlin\",\"norsho\",\n",
    "    \"parjae\",\"reccar\",\"ribgul\",\"saffin\",\"sooshe\",\"wantat1\",\"whttro\",\"akekee\",\n",
    "    \"arcter\",\"blkfra\",\"brtcur\",\"calqua\",\"chukar\",\"coopet\",\"gadwal\",\"gresca\",\n",
    "    \"hawhaw\",\"incter1\",\"lcspet\",\"madpet\",\"mitpar\",\"nutman\",\"pecsan\",\"redava\",\n",
    "    \"rinduc\",\"sander\",\"sooter1\",\"warwhe1\",\"wiltur\",\"akepa1\",\"barpet\",\"blknod\",\n",
    "    \"bubsan\",\"cangoo\",\"cintea\",\"crehon\",\"gamqua\",\"gryfra\",\"hawpet1\",\"jabwar\",\n",
    "    \"leasan\",\"magpet1\",\"moudov\",\"oahama\",\"peflov\",\"redjun\",\"rinphe\",\"semplo\",\n",
    "    \"sopsku1\",\"wesmea\",\"yebcar\",\"akiapo\",\"bcnher\",\"bongul\",\"buffle\",\"canvas\",\n",
    "    \"comgal1\",\"dunlin\",\"glwgul\",\"gwfgoo\",\"hoomer\",\"japqua\",\"leater1\",\"mallar3\",\n",
    "    \"norcar\",\"omao\",\"perfal\",\"redpha1\",\"rocpig\",\"sheowl\",\"sora\",\"wessan\",\"yefcan\",\n",
    "    \"akikik\",\"belkin1\",\"brant\",\"bulpet\",\"caster1\",\"commyn\",\"elepai\",\"gnwtea\",\n",
    "    \"hawama\",\"houfin\",\"kalphe\",\"lessca\",\"masboo\",\"norhar2\",\"osprey\",\"pibgre\",\n",
    "    \"refboo\",\"rorpar\",\"shtsan\",\"spodov\",\"wetshe\",\"zebdov\",\"amewig\",\"bkbplo\",\n",
    "    \"brnboo\",\"burpar\",\"categr\",\"compea\",\"ercfra\",\"golphe\",\"hawcoo\",\"houspa\",\n",
    "    \"kauama\",\"lesyel\",\"mauala\",\"normoc\",\"pagplo\",\"pomjae\",\"rempar\",\"rudtur\",\n",
    "    \"skylar\",\"sposan\",\"whfibi\",\"aniani\",\"bknsti\",\"brnnod\",\"buwtea\",\"chbsan\",\n",
    "    \"comsan\",\"eurwig\",\"grbher3\",\"hawcre\",\"hudgod\",\"laugul\",\"lobdow\",\"maupar\",\n",
    "    \"norpin\",\"palila\",\"puaioh\",\"rettro\",\"ruff\",\"snogoo\",\"towsol\",\"whiter\"\n",
    "]\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "hparams = {\n",
    "    \"lr\": 3e-3,\n",
    "    \"epochs\": 40,\n",
    "    \"batch_size\": 32,\n",
    "    \"data_len\": len(data_loader)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn import functional as F\n",
    "import torchmetrics\n",
    "\n",
    "class BirdCLEFModel(pl.LightningModule):\n",
    "    def __init__(self, backbone, input_shape, num_classes, loss_fn, learning_rate=2e-4):\n",
    "        super().__init__()\n",
    "\n",
    "        # log hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dim = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Loss function\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy()\n",
    "        self.val_accuracy = torchmetrics.Accuracy()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        n_sizes = backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Sequential()\n",
    "        # # layers are frozen by using eval()\n",
    "        # self.backbone.eval()\n",
    "        # # freeze params\n",
    "        # for param in self.backbone.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        self.backbone.fc = nn.Linear(n_sizes, num_classes)\n",
    "        \n",
    "        # n_sizes = self._get_conv_output(input_shape)\n",
    "        # self.classifier = nn.Linear(n_sizes, num_classes)\n",
    "\n",
    "    # returns the size of the output tensor going into the Linear layer from the conv block.\n",
    "    def _get_conv_output(self, shape):\n",
    "        batch_size = 1\n",
    "        tmp_input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
    "\n",
    "        output_feat = self._forward_features(tmp_input) \n",
    "        n_size = output_feat.data.view(batch_size, -1).size(1)\n",
    "        return n_size\n",
    "        \n",
    "    # returns the feature tensor from the conv block\n",
    "    def _forward_features(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "    \n",
    "    # will be used during inference\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # x = F.log_softmax(self.classifier(x), dim=1)\n",
    "        # x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        # loss = F.cross_entropy(y_hat, y)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        self.accuracy(y_hat, y)\n",
    "        self.log('train_acc_step', self.accuracy.compute(), prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def training_epoch_end(self, outs):\n",
    "        # log epoch metric\n",
    "        self.log('train_acc_epoch', self.accuracy.compute())\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        # loss = F.cross_entropy(y_hat, y)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('valid_loss', loss, on_step=True)\n",
    "        self.val_accuracy(y_hat, y)\n",
    "        self.log('val_acc_step', self.val_accuracy.compute(), prog_bar=True)\n",
    "    \n",
    "    def validation_epoch_end(self, outs):\n",
    "        # log epoch metric\n",
    "        self.log('val_acc_epoch', self.val_accuracy.compute())\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        # loss = F.cross_entropy(y_hat, y)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "        self.val_accuracy(y_hat, y)\n",
    "        self.log('test_acc_step', self.val_accuracy.compute())\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=self.learning_rate, momentum=0.9)\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            self.learning_rate,\n",
    "            steps_per_epoch=hparams[\"data_len\"],\n",
    "            epochs=hparams[\"epochs\"]\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\" : \"step\" }\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell\n",
    "# class LabelSmoothingCrossEntropy(torch.nn.Module):\n",
    "#     y_int = True\n",
    "#     def __init__(self, eps:float=0.1, weight=None, reduction='mean'):\n",
    "#         self.eps = eps\n",
    "#         self.weight = weight\n",
    "#         self.reduction = reduction\n",
    "\n",
    "#     def forward(self, output, target):\n",
    "#         c = output.size()[1]\n",
    "#         log_preds = F.log_softmax(output, dim=1)\n",
    "#         if self.reduction=='sum': loss = -log_preds.sum()\n",
    "#         else:\n",
    "#             loss = -log_preds.sum(dim=1) #We divide by that size at the return line so sum and not mean\n",
    "#             if self.reduction=='mean':  loss = loss.mean()\n",
    "#         return loss*self.eps/c + (1-self.eps) * F.nll_loss(log_preds, target.long(), weight=self.weight, reduction=self.reduction)\n",
    "\n",
    "#     # def activation(self, out): return F.softmax(out, dim=-1)\n",
    "#     # def decodes(self, out):    return out.argmax(dim=-1)\n",
    "\n",
    "def label_smoothing_cross_entropy_loss(output, target, eps=0.1, weight=None, reduction='mean'):\n",
    "        c = output.size()[1]\n",
    "        log_preds = F.log_softmax(output, dim=1)\n",
    "        if reduction=='sum': loss = -log_preds.sum()\n",
    "        else:\n",
    "            loss = -log_preds.sum(dim=1) #We divide by that size at the return line so sum and not mean\n",
    "            if reduction=='mean':  loss = loss.mean()\n",
    "        return loss*eps/c + (1-eps) * F.nll_loss(log_preds, target.long(), weight=weight, reduction=reduction)\n",
    "\n",
    "# from fastai.losses import LabelSmoothingCrossEntropy\n",
    "# loss_function = torch.nn.CrossEntropyLoss()\n",
    "# loss_function = loss_function.to(device)\n",
    "loss_function = label_smoothing_cross_entropy_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "backbone = models.resnet34(pretrained=True)\n",
    "model = BirdCLEFModel(\n",
    "    backbone=backbone,\n",
    "    input_shape=(3, 64, 313),\n",
    "    num_classes=len(class_names),\n",
    "    loss_fn=loss_function,\n",
    "    learning_rate=hparams[\"lr\"]\n",
    ")\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Finding best initial lr: 100%|██████████| 100/100 [01:03<00:00,  2.50it/s]Restoring states from the checkpoint path at /home/kitemetric/workspace/kaggle-challenges/BirdCLEF-2022/notebooks/lr_find_temp_model_18b90962-6bba-42fa-a0fb-664628513a3a.ckpt\n",
      "Finding best initial lr: 100%|██████████| 100/100 [01:08<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested LR: 0.0022908676527677745\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Init ModelCheckpoint callback, monitoring 'val_loss'\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        monitor='valid_loss',\n",
    "        filename='birdclef-resnet34-epoch{epoch:02d}-val_loss{valid_loss:.4f}',\n",
    "        auto_insert_metric_name=False\n",
    "    ),\n",
    "    EarlyStopping(monitor=\"valid_loss\", patience=5, verbose=False)\n",
    "]\n",
    "  \n",
    "trainer = pl.trainer.Trainer(\n",
    "    max_epochs=hparams[\"epochs\"],\n",
    "    gpus=1,\n",
    "    callbacks=callbacks,\n",
    "    accelerator=\"gpu\",\n",
    "    auto_lr_find=True,\n",
    "    enable_checkpointing=True\n",
    ")\n",
    "\n",
    "# Run learning rate finder\n",
    "lr_finder = trainer.tuner.lr_find(model, train_dataloaders=data_loader)\n",
    "\n",
    "# Results can be found in\n",
    "lr_finder.results\n",
    "\n",
    "# Plot with\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()\n",
    "\n",
    "# Pick point based on plot, or get suggestion\n",
    "new_lr = lr_finder.suggestion()\n",
    "print(f\"Suggested LR: {new_lr}\")\n",
    "\n",
    "# update hparams of the model\n",
    "model.learning_rate = new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name         | Type     | Params\n",
      "------------------------------------------\n",
      "0 | accuracy     | Accuracy | 0     \n",
      "1 | val_accuracy | Accuracy | 0     \n",
      "2 | backbone     | ResNet   | 21.4 M\n",
      "------------------------------------------\n",
      "21.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.4 M    Total params\n",
      "85.451    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  58%|█████▊    | 269/465 [02:42<01:58,  1.65it/s, loss=2.4, v_num=26, train_acc_step=0.301, val_acc_step=0.289]"
     ]
    }
   ],
   "source": [
    "# CHECKPOINT = \"version_22-ep20-0.4958_val.ckpt\"\n",
    "trainer.fit(model, data_loader, val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, test_dataloaders=val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(\"version_22-ep20-0.4958_val.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4ea5d56035456c2b1b6b90ed23f43434caf1e4d5f44691eca1785db1c6f7643"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
